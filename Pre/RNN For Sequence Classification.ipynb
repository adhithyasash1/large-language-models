{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Topic :** RNN for Sequence Classification <br>\n",
        "**Author:** Arun Prakash A"
      ],
      "metadata": {
        "id": "mg6IP5XhqXf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary libraries"
      ],
      "metadata": {
        "id": "eFFi3eM7Qe8L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_X0di_RQZGj",
        "outputId": "567cc4d8-e827-45f7-fa8c-5aca270b45a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdata==0.6.0\n",
            "  Downloading torchdata-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0) (2.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0) (2.31.0)\n",
            "Collecting torch==2.0.0 (from torchdata==0.6.0)\n",
            "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.1.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchdata==0.6.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchdata==0.6.0) (0.41.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchdata==0.6.0) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchdata==0.6.0) (16.0.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.0) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->torchdata==0.6.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->torchdata==0.6.0) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchdata\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.6.1\n",
            "    Uninstalling torchdata-0.6.1:\n",
            "      Successfully uninstalled torchdata-0.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchtext 0.15.2 requires torchdata==0.6.1, but you have torchdata 0.6.0 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 torchdata-0.6.0\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchdata==0.6.0 # to be compatible with torch 2.0\n",
        "!pip install portalocker==2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "#torch specific\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Data loader\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "\n",
        "#text lib\n",
        "import torchtext\n",
        "\n",
        "#fetch data\n",
        "from torchtext.datasets import AG_NEWS\n",
        "\n",
        "# tokenizer\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "#build vocabulary\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# get input_ids (numericalization)\n",
        "from torchtext.transforms import VocabTransform\n",
        "\n",
        "# get embeddings\n",
        "from torch.nn import Embedding\n",
        "\n",
        "# get rnn model and layers\n",
        "from torch.nn import RNN, Linear, Sigmoid, Softmax\n",
        "\n",
        "# optimizer\n",
        "import torch.optim as optim\n",
        "\n",
        "# utils\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.nn.utils.rnn import pad_packed_sequence"
      ],
      "metadata": {
        "id": "hIMSZ4yLRE3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "0NgYqGDXyfP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data"
      ],
      "metadata": {
        "id": "kmRZVlYHXVE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./data',exist_ok=True)\n",
        "train_samples = AG_NEWS(root='./data',split='train')\n",
        "print('Number of training samples: ',len(list(train_samples)))\n",
        "print('A sample: \\n',next(iter(train_samples)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAnv8eUlXQ0-",
        "outputId": "9170ff25-f62c-475c-f632-8889f9b062a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples:  120000\n",
            "A sample: \n",
            " (3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "wjMSIG3SX0Nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(tokenizer=\"basic_english\",language='en')"
      ],
      "metadata": {
        "id": "i84zUWhIXzn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ['This is called tokenization!','this is not the best approach by the way']\n",
        "token_list = [tokenizer(sentence) for sentence in text]\n",
        "print(token_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZykeiTOYXR-",
        "outputId": "d5f2fc7d-b8fa-4022-c6e9-23b7d018b8dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['this', 'is', 'called', 'tokenization', '!'], ['this', 'is', 'not', 'the', 'best', 'approach', 'by', 'the', 'way']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# token iterator\n",
        "def yield_tokens(corpus):\n",
        "  for (label,sentence) in corpus:\n",
        "    yield tokenizer(sentence)"
      ],
      "metadata": {
        "id": "IWGEJ4WwY0l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Vocabulary"
      ],
      "metadata": {
        "id": "DLjgSlx6ZBTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v = build_vocab_from_iterator(yield_tokens(train_samples),min_freq=100,specials=['<pad>','<unk>'])\n",
        "v.set_default_index(v['<unk>']) # index of OOV"
      ],
      "metadata": {
        "id": "s1wLAD7gY_yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(v['deep'],v['learning'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOwqhVTPaDpy",
        "outputId": "e2169f0b-8023-4d42-cd24-c66504b76351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2162 4700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Numericalization"
      ],
      "metadata": {
        "id": "A8Ym7woXaZbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_transform = VocabTransform(v)\n",
        "\n",
        "for sample in train_samples:\n",
        "  input_ids = vocab_transform(tokenizer(sample[1])) # 0th index is a label\n",
        "  print(input_ids)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1EPXlRdaYwa",
        "outputId": "473599f8-2332-4795-9b13-b62c15451b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[432, 426, 2, 1606, 1, 114, 67, 3, 849, 14, 28, 15, 28, 16, 1, 4, 432, 375, 17, 10, 1, 7, 1, 4, 43, 4010, 784, 326, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_ids(sample):\n",
        "  tokens = tokenizer(sample[1]) # again, oth index is a label\n",
        "  return torch.LongTensor(vocab_transform(tokens))"
      ],
      "metadata": {
        "id": "C0-Q7rH2cndl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "i99VnU00cPXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = Embedding(num_embeddings = len(v),embedding_dim=6,padding_idx=0)"
      ],
      "metadata": {
        "id": "uZfPZizcaJXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in train_samples:\n",
        "  input_ids = get_input_ids(sample)\n",
        "  print(input_ids)\n",
        "  print(embedding(input_ids))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDOJ4FHBdBtZ",
        "outputId": "18d9364f-eb13-44a5-d85d-892b7be0b672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 432,  426,    2, 1606,    1,  114,   67,    3,  849,   14,   28,   15,\n",
            "          28,   16,    1,    4,  432,  375,   17,   10,    1,    7,    1,    4,\n",
            "          43, 4010,  784,  326,    2])\n",
            "tensor([[ 2.3562, -0.5289, -0.7235,  0.1099,  1.5542, -0.0472],\n",
            "        [-0.2429,  0.1881, -0.0528,  1.3146, -0.3414, -0.6835],\n",
            "        [-0.3804, -0.3066,  1.0414,  0.5904, -1.4058, -0.5009],\n",
            "        [ 1.1506, -1.3453, -1.3722,  0.1924, -0.6143,  2.3126],\n",
            "        [ 1.2393, -1.5741, -0.1816, -0.0509, -0.4694, -0.8691],\n",
            "        [ 0.0959, -0.2027, -1.1928,  0.7694,  0.1288,  0.6164],\n",
            "        [-0.2400, -0.5430, -1.2325, -0.1354,  0.1389,  0.2343],\n",
            "        [-0.4732, -0.4493,  1.0912,  0.2197,  0.4803,  0.2490],\n",
            "        [-0.7445,  1.0203, -0.3015,  1.5908,  0.1232,  0.6188],\n",
            "        [-0.5888,  2.6468,  0.1932,  0.8338, -0.3670,  0.1345],\n",
            "        [ 0.5190,  1.4543, -1.9487, -0.4160,  0.3626, -0.0891],\n",
            "        [ 0.2368,  0.1440,  0.5376,  0.1592, -0.7456,  1.1223],\n",
            "        [ 0.5190,  1.4543, -1.9487, -0.4160,  0.3626, -0.0891],\n",
            "        [-0.8503, -0.1219, -0.5158,  1.2260,  0.4766,  0.9146],\n",
            "        [ 1.2393, -1.5741, -0.1816, -0.0509, -0.4694, -0.8691],\n",
            "        [-0.1791,  0.5769, -0.3353,  1.0632,  1.5667, -2.3713],\n",
            "        [ 2.3562, -0.5289, -0.7235,  0.1099,  1.5542, -0.0472],\n",
            "        [-0.2545,  1.5167, -0.8235,  1.9493, -0.3530,  0.8570],\n",
            "        [-0.0961,  0.3146,  1.1558, -1.4127,  0.9615, -0.1756],\n",
            "        [ 1.2192, -0.9969, -0.6706,  2.2062,  0.0922, -0.0395],\n",
            "        [ 1.2393, -1.5741, -0.1816, -0.0509, -0.4694, -0.8691],\n",
            "        [-0.3858, -0.0194, -0.1396, -0.2212,  0.7771,  0.7454],\n",
            "        [ 1.2393, -1.5741, -0.1816, -0.0509, -0.4694, -0.8691],\n",
            "        [-0.1791,  0.5769, -0.3353,  1.0632,  1.5667, -2.3713],\n",
            "        [ 0.5133,  0.3768, -0.4954,  0.1033, -1.2052,  0.0985],\n",
            "        [ 1.3961,  0.4016, -1.4848, -1.4230, -1.3915,  1.1813],\n",
            "        [ 0.5836, -0.5254, -0.0643,  0.9816,  0.2175,  0.0732],\n",
            "        [-0.6740,  1.9275,  0.7240,  2.2239, -1.1535, -0.6911],\n",
            "        [-0.3804, -0.3066,  1.0414,  0.5904, -1.4058, -0.5009]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoading"
      ],
      "metadata": {
        "id": "dnKKLinOenIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in train_samples:\n",
        "  input_ids = get_input_ids(sample)\n",
        "  print(input_ids.shape)\n",
        "  prompt = input('Continue?')\n",
        "  if prompt == 'y':\n",
        "    continue\n",
        "  else:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTZ2zAn_es9R",
        "outputId": "cfeda8a8-5471-481f-c920-fb6032e1bce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([29])\n",
            "Continue?y\n",
            "torch.Size([42])\n",
            "Continue?n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The length of sequences is different for each sample.\n",
        "* However, Batching tensors requires tensors to be of same length\n",
        "* So we need to pad sequences to the maximum len of sequence in a batch"
      ],
      "metadata": {
        "id": "n1I-KgaNfSIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [(1,'padding is necessary'),(4,'you know the reason right?')]\n",
        "batch_input_ids = [get_input_ids(sample) for sample in examples ]\n",
        "padded_input_ids = pad_sequence(batch_input_ids,batch_first=True,padding_value=0.0)"
      ],
      "metadata": {
        "id": "526pz5w-f6bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(padded_input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4-CRRjKjexV",
        "outputId": "5d918789-2181-41f9-8216-fd8ce19941bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   1,   22, 4425,    0,    0,    0],\n",
            "        [ 166, 1200,    3, 2257,  480,   81]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define collate function to be passed to a DataLoader\n",
        "\n",
        " * The output of the function is a tuple containing` (label tensor,padded_seq,length of unpadded sequence)`\n",
        " * We see the requirement for length info later"
      ],
      "metadata": {
        "id": "sKEX3p3HkcV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_function(batch_samples):\n",
        "  '''\n",
        "  Input : Sample : (label,sentence)\n",
        "  return :  (label tensor, padded_seuence ,lengths of unpadded seq in batches)\n",
        "  '''\n",
        "\n",
        "  #padded_seq\n",
        "  batch_input_ids = [get_input_ids(sample) for sample in batch_samples ]\n",
        "\n",
        "  padded_input_ids = pad_sequence(batch_input_ids,batch_first=True,padding_value=0.0)\n",
        "\n",
        "  # label tensor\n",
        "  # -1 is added to make class num starting from 0, required for one-hot encoding\n",
        "  labels = torch.tensor([torch.LongTensor([sample[0]-1]) for sample in batch_samples])\n",
        "\n",
        "  # lengths of unpadded seq\n",
        "\n",
        "  lengths = [len(tokenizer(sample[1]))for sample in batch_samples]\n",
        "\n",
        "  return (labels,padded_input_ids,lengths)"
      ],
      "metadata": {
        "id": "6UF5Z67djg6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label,sample,lengths = collate_function([(1,'this is great'),(2,'why is this taking such a long time?')])\n",
        "print('label tensor: \\n ',label)\n",
        "print('Padded sequence: \\n',sample)\n",
        "print('Actual lengths: ', lengths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78DFzNcKlk1P",
        "outputId": "e4fa139e-0c87-4f68-8ad3-57dac41d6f87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label tensor: \n",
            "  tensor([0, 1])\n",
            "Padded sequence: \n",
            " tensor([[  53,   22,  811,    0,    0,    0,    0,    0,    0],\n",
            "        [1165,   22,   53,  608,  560,    6,  443,  102,   81]])\n",
            "Actual lengths:  [3, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create RNN model"
      ],
      "metadata": {
        "id": "2e7dQYBQm7Xp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* During the forward pass, the embedding layer takes in `padded_input_ids` and returns embedding vectors for each of index including padded index (0)\n",
        "* However, while instantiating embed layer, we let the layer know that \"Hey, **0**s in `padded_input_ids` is just padded values and therefore do not wait for gradients for the embeddings, just move on without raising an error\"\n",
        "\n",
        "* Moreover, we use `pack_padded_sequence` in the forward method to avoid unnecessary computation for padded tokens. Please refer to the documentation to know how.\n",
        "\n",
        " * Some good discussions at : https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch"
      ],
      "metadata": {
        "id": "kCfAbIXTnPzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNClassifier(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(vocab_size, embed_dim,padding_idx=0)\n",
        "        self.rnn = RNN(embed_dim,hidden_dim,batch_first=True)\n",
        "        self.fc = Linear(hidden_dim, num_class)\n",
        "\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # get embedding for padded sequence\n",
        "        x = self.embedding(x)\n",
        "        x = pack_padded_sequence(x,lengths=lengths,enforce_sorted=False,batch_first=True)\n",
        "\n",
        "        # get hidden states for all time steps, last time step h_T as packed sequence\n",
        "        x = self.rnn(x)\n",
        "        # get the final state h_T\n",
        "        x = self.fc(x[1])  # logits\n",
        "        return x"
      ],
      "metadata": {
        "id": "iLEfcwdDmSDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "etHVUf2Tpa3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "dataloader = DataLoader(train_samples,batch_size=batch_size,collate_fn = collate_function,shuffle=True)"
      ],
      "metadata": {
        "id": "2U8izvhNpGEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(v)\n",
        "embedding_dim = 300\n",
        "num_classes = 4\n",
        "hidden_dim = 60\n",
        "model = RNNClassifier(vocab_size,embedding_dim,hidden_dim,num_classes)"
      ],
      "metadata": {
        "id": "U48ZqpWEpiai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "Ap1TgBi8KDhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for y,x,lengths in dataloader:\n",
        "  print(y)\n",
        "  pred = model(x.to(device),lengths)\n",
        "  print('Logits: ',pred.squeeze())\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzAUYXzkp2ER",
        "outputId": "8cc967c2-962b-4fe5-8e86-9c750b66d217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2])\n",
            "Logits:  tensor([[-0.5244, -0.1885,  0.4447,  0.0809],\n",
            "        [-0.2737, -0.0047,  0.3406, -0.0866],\n",
            "        [-0.6832,  0.0593,  0.4331,  0.0663],\n",
            "        [-0.3094, -0.0596,  0.3702,  0.0261],\n",
            "        [-0.6537, -0.0932,  0.1200, -0.0056],\n",
            "        [-0.4887, -0.4386, -0.8866,  0.1748],\n",
            "        [-0.2316,  0.1557,  0.3573, -0.1122],\n",
            "        [-0.3872,  0.0068,  0.3042,  0.0156],\n",
            "        [-0.7005,  0.1307,  0.4297,  0.0455],\n",
            "        [-0.5245, -0.1886,  0.4448,  0.0808],\n",
            "        [-0.6746, -0.0570,  0.4859,  0.0015],\n",
            "        [-0.3526, -0.2338,  0.1421, -0.0042],\n",
            "        [-0.6015,  0.3427,  0.1239, -0.1163],\n",
            "        [-0.5070, -0.1952,  0.3351,  0.2003],\n",
            "        [-0.5477,  0.0875,  0.2805,  0.0417],\n",
            "        [-0.3720, -0.0570,  0.4535,  0.0223],\n",
            "        [-0.5821, -0.0288,  0.4952,  0.0796],\n",
            "        [-0.5940, -0.0900,  0.3068, -0.2462],\n",
            "        [-0.3725, -0.1550,  0.1942,  0.1390],\n",
            "        [-0.2674,  0.2648,  0.3617, -0.1433],\n",
            "        [-0.5210,  0.2501,  0.2264, -0.2186],\n",
            "        [-0.3483,  0.2454,  0.2528, -0.1061],\n",
            "        [-0.5152, -0.3092,  0.3519,  0.0587],\n",
            "        [-0.1950,  0.1973,  0.2768,  0.0408],\n",
            "        [-0.5234,  0.1632,  0.2439,  0.0086],\n",
            "        [-0.4702,  0.0967,  0.2452, -0.0800],\n",
            "        [-0.3189, -0.0447,  0.0281, -0.1020],\n",
            "        [-0.6425, -0.1971,  0.2267,  0.2486],\n",
            "        [-0.6815, -0.0329,  0.2013,  0.1990],\n",
            "        [-0.5176, -0.0023,  0.1170, -0.0335],\n",
            "        [-0.3654,  0.1663,  0.2109,  0.1511],\n",
            "        [-0.2658, -0.0243,  0.3814,  0.0478]], device='cuda:0',\n",
            "       grad_fn=<SqueezeBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Loss = nn.functional.cross_entropy\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "9AbP9DoarbkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import one_hot\n",
        "for epoch in range(1):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        labels, samples,lengths = data\n",
        "        labels_ohe = torch.tensor(one_hot(labels,num_classes=4),dtype=torch.float32)\n",
        "        labels_ohe = labels_ohe.to(device)\n",
        "        samples = samples.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(samples,lengths)\n",
        "\n",
        "        loss = Loss(outputs.squeeze(), labels_ohe.squeeze())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "\n",
        "        class_correct = torch.argmax(outputs.to('cpu'),axis=2) == torch.as_tensor(labels)\n",
        "        running_acc += torch.count_nonzero(class_correct)/batch_size\n",
        "        if i % 100 == 99:    # print every 10000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f Accuracy:%.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 99,running_acc/99))\n",
        "            running_loss = 0.0\n",
        "            running_acc = 0.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqaluRoRrc7n",
        "outputId": "87d07535-6c0e-44bb-9d60-0b2e9634ddbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-865360a90b44>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels_ohe = torch.tensor(one_hot(labels,num_classes=4),dtype=torch.float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   100] loss: 1.108 Accuracy:0.533\n",
            "[1,   200] loss: 0.929 Accuracy:0.598\n",
            "[1,   300] loss: 0.890 Accuracy:0.625\n",
            "[1,   400] loss: 1.191 Accuracy:0.510\n",
            "[1,   500] loss: 0.934 Accuracy:0.606\n",
            "[1,   600] loss: 0.839 Accuracy:0.660\n",
            "[1,   700] loss: 0.791 Accuracy:0.682\n",
            "[1,   800] loss: 0.744 Accuracy:0.712\n",
            "[1,   900] loss: 0.798 Accuracy:0.678\n",
            "[1,  1000] loss: 0.776 Accuracy:0.700\n",
            "[1,  1100] loss: 0.749 Accuracy:0.693\n",
            "[1,  1200] loss: 0.715 Accuracy:0.701\n",
            "[1,  1300] loss: 0.730 Accuracy:0.712\n",
            "[1,  1400] loss: 0.713 Accuracy:0.710\n",
            "[1,  1500] loss: 0.683 Accuracy:0.733\n",
            "[1,  1600] loss: 0.696 Accuracy:0.721\n",
            "[1,  1700] loss: 0.689 Accuracy:0.730\n",
            "[1,  1800] loss: 0.716 Accuracy:0.720\n",
            "[1,  1900] loss: 0.744 Accuracy:0.709\n",
            "[1,  2000] loss: 0.700 Accuracy:0.729\n",
            "[1,  2100] loss: 0.760 Accuracy:0.712\n",
            "[1,  2200] loss: 0.658 Accuracy:0.751\n",
            "[1,  2300] loss: 0.741 Accuracy:0.711\n",
            "[1,  2400] loss: 0.662 Accuracy:0.752\n",
            "[1,  2500] loss: 0.760 Accuracy:0.716\n",
            "[1,  2600] loss: 0.695 Accuracy:0.736\n",
            "[1,  2700] loss: 0.706 Accuracy:0.743\n",
            "[1,  2800] loss: 0.643 Accuracy:0.769\n",
            "[1,  2900] loss: 0.664 Accuracy:0.764\n",
            "[1,  3000] loss: 0.667 Accuracy:0.743\n",
            "[1,  3100] loss: 0.700 Accuracy:0.747\n",
            "[1,  3200] loss: 0.678 Accuracy:0.741\n",
            "[1,  3300] loss: 0.648 Accuracy:0.753\n",
            "[1,  3400] loss: 0.672 Accuracy:0.738\n",
            "[1,  3500] loss: 0.722 Accuracy:0.723\n",
            "[1,  3600] loss: 0.862 Accuracy:0.698\n",
            "[1,  3700] loss: 0.703 Accuracy:0.735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# inference (fun)"
      ],
      "metadata": {
        "id": "-1x8ut8LOLa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"all the focus is now on the biggest T20 league in the world \""
      ],
      "metadata": {
        "id": "whmheAqKzF_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_ids_inf(text):\n",
        "  tokens = tokenizer(text)\n",
        "  input_ids = vocab_transform(tokens)\n",
        "  return torch.LongTensor(input_ids).unsqueeze(0)"
      ],
      "metadata": {
        "id": "vzGJv0WYQ3ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_input_ids_inf(text).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pgsCaSBRuH_",
        "outputId": "39523200-e532-475f-c428-87e80d26958e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_inference = model.to('cpu')\n",
        "with torch.inference_mode(True):\n",
        "  logits = model_inference(get_input_ids_inf(text),[len(tokenizer(text))])\n",
        "  print(torch.nn.functional.softmax(logits,dim=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMKs8co6AbDh",
        "outputId": "daef7df2-83a8-4aec-8c63-9660e0e1d228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.0418, 0.6990, 0.0538, 0.2055]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TqMN7WyNSLRq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}