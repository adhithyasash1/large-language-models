{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPclBh9cjhlQyhy3nDxdDEI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Basics"],"metadata":{"id":"3gNFRqVyeXV4"}},{"cell_type":"markdown","source":["Let's go through each of these PyTorch functions to understand their uses and differences:\n","\n","### `torch.matmul`\n","\n","`torch.matmul` performs matrix multiplication between two tensors. The behavior depends on the dimensionality of the tensors:\n","- For two 1-D tensors, it computes their dot product.\n","- For two 2-D tensors, it computes their matrix multiplication.\n","- For higher-dimensional tensors, it performs batched matrix multiplication, treating the extra dimensions as batch dimensions.\n","\n","Example:\n","```python\n","a = torch.tensor([[1, 2], [3, 4]])\n","b = torch.tensor([[5, 6], [7, 8]])\n","result = torch.matmul(a, b)\n","```\n","\n","### `torch.bmm`\n","\n","`torch.bmm` performs batch matrix multiplication between two 3-D tensors. Each tensor's first dimension is considered as the batch size, and `torch.bmm` performs matrix multiplication for each pair of matrices in the batch.\n","\n","Example:\n","```python\n","a = torch.rand(3, 2, 5)  # 3 matrices of shape 2x5\n","b = torch.rand(3, 5, 4)  # 3 matrices of shape 5x4\n","result = torch.bmm(a, b)  # 3 matrices of shape 2x4\n","```\n","\n","### `torch.swapdims` (or `torch.transpose` in versions prior to 1.8.0)\n","\n","`torch.swapdims` swaps two dimensions of a tensor. It's useful for reordering the dimensions of a tensor without changing its data.\n","\n","Example:\n","```python\n","x = torch.randn(2, 3, 5)\n","result = torch.swapdims(x, 0, 1)  # Swaps the first and second dimension\n","```\n","\n","### `torch.unsqueeze`\n","\n","`torch.unsqueeze` adds a dimension of size one at a specified position in the tensor's shape. It's useful for increasing the dimensionality of a tensor, often for aligning tensor shapes for broadcasting.\n","\n","Example:\n","```python\n","x = torch.tensor([1, 2, 3])\n","result = torch.unsqueeze(x, 0)  # Result shape is now (1, 3)\n","```\n","\n","### `torch.squeeze`\n","\n","`torch.squeeze` removes all dimensions of size one from the tensor's shape. If a specific dimension is specified, it removes that dimension only if it is of size one.\n","\n","Example:\n","```python\n","x = torch.rand(1, 3, 1, 5)\n","result = torch.squeeze(x)  # Result shape is now (3, 5)\n","```\n","\n","### `torch.argmax`\n","\n","`torch.argmax` returns the indices of the maximum value of all elements in the input tensor. It can also operate along a specified dimension.\n","\n","Example:\n","```python\n","x = torch.tensor([[1, 2, 3], [4, 3, 2]])\n","result = torch.argmax(x)  # Returns the index of the overall max value, which is 3 (flattened index)\n","result_dim = torch.argmax(x, dim=1)  # Returns the indices of max values along dim 1, which is [2, 0]\n","```\n","\n","Each of these functions serves a specific purpose in tensor manipulation, from performing mathematical operations to altering the shape or dimensionality of tensors, which are fundamental in developing and manipulating models in PyTorch."],"metadata":{"id":"ahydA3QKkTu5"}},{"cell_type":"code","source":["a = torch.tensor([[1, 2], [3, 4]])\n","b = torch.tensor([[5, 6], [7, 8]])\n","result = torch.matmul(a, b)\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FFv7TcnPkdFw","executionInfo":{"status":"ok","timestamp":1708081211411,"user_tz":-330,"elapsed":3,"user":{"displayName":"SASHI ADHITHYA R","userId":"13689357405391493267"}},"outputId":"dd7a775b-4874-4d29-ce92-8c19ccf5e717"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[19, 22],\n","        [43, 50]])"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["a = torch.rand(3, 2, 5)  # 3 matrices of shape 2x5\n","b = torch.rand(3, 5, 4)  # 3 matrices of shape 5x4\n","result = torch.bmm(a, b)  # 3 matrices of shape 2x4\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PqHJI5C2kdl_","executionInfo":{"status":"ok","timestamp":1708081213022,"user_tz":-330,"elapsed":2,"user":{"displayName":"SASHI ADHITHYA R","userId":"13689357405391493267"}},"outputId":"31ff258a-c235-4c26-ceae-781f33942e9f"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.9343, 1.5857, 1.6803, 0.6953],\n","         [0.7715, 1.1902, 1.1333, 0.3764]],\n","\n","        [[1.7452, 1.3827, 1.5974, 0.9578],\n","         [1.7699, 1.7691, 1.9269, 1.1248]],\n","\n","        [[0.6482, 0.5338, 0.5232, 1.0784],\n","         [0.8460, 0.8159, 0.5877, 1.6714]]])"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["x = torch.randn(2, 3, 5)\n","result = torch.swapdims(x, 0, 1)  # Swaps the first and second dimension\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qKORuTuBkfB3","executionInfo":{"status":"ok","timestamp":1708081215285,"user_tz":-330,"elapsed":447,"user":{"displayName":"SASHI ADHITHYA R","userId":"13689357405391493267"}},"outputId":"11622e54-62d7-45bf-f25f-6b456702835a"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0.5636, -1.5072, -1.6107, -1.4790,  0.4323],\n","         [ 0.9733, -1.0151, -0.5419, -0.4410, -0.3136]],\n","\n","        [[-0.1250,  0.7821,  0.5635, -0.1091,  0.7152],\n","         [-0.1293, -0.7150,  2.1698,  2.0207,  0.2539]],\n","\n","        [[ 0.0391,  1.3059,  0.2466, -1.9776,  0.3370],\n","         [ 0.9364,  0.7122, -0.0318,  0.1016,  1.3433]]])"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["x = torch.tensor([1, 2, 3])\n","result = torch.unsqueeze(x, 0)  # Result shape is now (1, 3)\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"inx2zi5_kiJg","executionInfo":{"status":"ok","timestamp":1708081215705,"user_tz":-330,"elapsed":2,"user":{"displayName":"SASHI ADHITHYA R","userId":"13689357405391493267"}},"outputId":"fd3d90dc-5e3f-433a-be96-a230df694c24"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 2, 3]])"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["x = torch.rand(1, 3, 1, 5)\n","result = torch.squeeze(x)  # Result shape is now (3, 5)\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kW_VcPYQkj5Q","executionInfo":{"status":"ok","timestamp":1708081217951,"user_tz":-330,"elapsed":1,"user":{"displayName":"SASHI ADHITHYA R","userId":"13689357405391493267"}},"outputId":"0f57d862-a077-4b58-abdc-47045ebb3d7b"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.2606, 0.0931, 0.9193, 0.2999, 0.6325],\n","        [0.3265, 0.5406, 0.9662, 0.7304, 0.0667],\n","        [0.6985, 0.9746, 0.6315, 0.8352, 0.9929]])"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["x = torch.tensor([[1, 2, 3], [4, 3, 2]])\n","result = torch.argmax(x)  # Returns the index of the overall max value, which is 3 (flattened index)\n","result_dim = torch.argmax(x, dim=1)  # Returns the indices of max values along dim 1, which is [2, 0]\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OdpS4vKokk9I","executionInfo":{"status":"ok","timestamp":1708081220130,"user_tz":-330,"elapsed":2,"user":{"displayName":"SASHI ADHITHYA R","userId":"13689357405391493267"}},"outputId":"3fede412-f898-4ff5-ad99-706fefa7c21b"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(3)"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["`torch.sum` is a PyTorch function that computes the sum of all elements in the input tensor, or along a specified dimension if given. It is a versatile function that can be used for reducing tensors by summing their elements, which is useful in various mathematical and neural network operations.\n","\n","### Syntax\n","\n","The basic syntax of `torch.sum` is as follows:\n","\n","```python\n","torch.sum(input, dim=None, keepdim=False, dtype=None) -> Tensor\n","```\n","\n","- **`input`** (Tensor): The input tensor whose elements you want to sum.\n","- **`dim`** (int or tuple of python:ints, optional): The dimension or dimensions along which the elements will be summed. If not specified, the sum of all elements will be returned.\n","- **`keepdim`** (bool, optional): Whether the output tensor has `dim` retained or not. If `True`, the output tensor will have the same number of dimensions as the input, with the length of 1 in the reduced dimensions. Default is `False`, which reduces the dimension.\n","- **`dtype`** (torch.dtype, optional): The desired data type of the returned tensor. If specified, the input tensor is casted to `dtype` before performing the operation. Default is `None`, which infers the dtype from the input tensor.\n","\n","### Examples\n","\n","#### Sum of All Elements\n","\n","```python\n","import torch\n","\n","a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","total_sum = torch.sum(a)\n","print(total_sum)  # Output: tensor(21)\n","```\n","\n","#### Sum Along a Specific Dimension\n","\n","```python\n","column_sum = torch.sum(a, dim=0)  # Sum along columns\n","print(column_sum)  # Output: tensor([5, 7, 9])\n","\n","row_sum = torch.sum(a, dim=1)  # Sum along rows\n","print(row_sum)  # Output: tensor([ 6, 15])\n","```\n","\n","#### Keeping Dimension After Sum\n","\n","```python\n","column_sum_keepdim = torch.sum(a, dim=0, keepdim=True)\n","print(column_sum_keepdim)  # Output: tensor([[5, 7, 9]])\n","\n","row_sum_keepdim = torch.sum(a, dim=1, keepdim=True)\n","print(row_sum_keepdim)  # Output: tensor([[ 6], [15]])\n","```\n","\n","### Use Cases\n","\n","`torch.sum` is widely used in neural network operations, such as computing loss functions, normalizing data, implementing custom layers or functions, and aggregating model outputs. It's a fundamental operation for tensor manipulation and analysis in PyTorch."],"metadata":{"id":"Gc4Vurq20-oV"}},{"cell_type":"code","source":["import torch\n","\n","a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","total_sum = torch.sum(a)\n","print(total_sum)  # Output: tensor(21)"],"metadata":{"id":"xNSJdAIE1CuN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["column_sum = torch.sum(a, dim=0)  # Sum along columns\n","print(column_sum)  # Output: tensor([5, 7, 9])\n","\n","row_sum = torch.sum(a, dim=1)  # Sum along rows\n","print(row_sum)  # Output: tensor([ 6, 15])"],"metadata":{"id":"HdJbOen21Ddv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["column_sum_keepdim = torch.sum(a, dim=0, keepdim=True)\n","print(column_sum_keepdim)  # Output: tensor([[5, 7, 9]])\n","\n","row_sum_keepdim = torch.sum(a, dim=1, keepdim=True)\n","print(row_sum_keepdim)  # Output: tensor([[ 6], [15]])"],"metadata":{"id":"OLDYyPCd1FDV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Manipulate an image using PyTorch\n","\n","1. **Load the Image**: You're attempting to load an image from a URL. This part cannot be executed in this environment, but the approach is correct for a typical Python environment with internet access.\n","\n","2. **Convert to NumPy Array**: Then, you convert the PIL image to a NumPy array. This is a common practice when working with images in Python, as it allows for easier manipulation.\n","\n","3. **Convert to PyTorch Tensor**: Next, you convert the NumPy array to a PyTorch tensor. This is useful for performing tensor operations using PyTorch.\n","\n","4. **Print Tensor Shape**: You print the shape of the tensor to understand its dimensions, which typically are `(height, width, channels)` for an image.\n","\n","5. **Transpose the Image**: You attempt to transpose the image using `torch.transpose`, swapping the height and width dimensions. However, this operation does not include the color channels dimension, which might lead to an error or unexpected behavior when visualizing the image.\n","\n","6. **Permute the Image for Visualization**: Lastly, you use `torch.permute` to correctly swap the dimensions, including the color channels dimension, to ensure the image can be visualized correctly. The correct way to transpose an image tensor including the color channel is indeed using `torch.permute`.\n","\n","Here's the corrected approach to transpose an image tensor and visualize it using Matplotlib, assuming `t_img` is your image tensor:\n","\n","```python\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","\n","# Assuming t_img is your image tensor with shape [H, W, C]\n","\n","# Correctly transpose the image tensor to [W, H, C] for visualization\n","t_img_T = torch.permute(t_img, (1, 0, 2))\n","\n","# Convert the transposed tensor to a NumPy array and visualize\n","plt.imshow(t_img_T.numpy())\n","plt.show()\n","```\n","\n","The key takeaway is to use `torch.permute` for correctly reordering the dimensions of an image tensor when you need to include the color channels in the transposition."],"metadata":{"id":"UItBhS_z2jrN"}},{"cell_type":"code","source":["from PIL import Image\n","import requests\n","from io import BytesIO\n","import numpy as np\n","from matplotlib import pyplot as plt"],"metadata":{"id":"4zK_IoF32kyT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(img)\n","plt.show()"],"metadata":{"id":"RMITNNId2lVF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t_img = torch.tensor(img)\n","print(t_img.shape)"],"metadata":{"id":"gcfxajmw2nMV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t_img_T =  torch.transpose(t_img,dim0=1,dim1=0)\n","plt.imshow(np.array(t_img_T))\n","plt.show()"],"metadata":{"id":"wJGNwoQm2pNG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t_img_T =  torch.permute(t_img,(1,0,2))\n","plt.imshow(np.array(t_img_T))\n","plt.show()"],"metadata":{"id":"uQGrLA4k2pPl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Autograds"],"metadata":{"id":"t5HUVD5NENn1"}},{"cell_type":"markdown","source":[],"metadata":{"id":"b4ahXSKDEQtP"}},{"cell_type":"markdown","source":["# Embeddings"],"metadata":{"id":"q-53EVMKgl3w"}},{"cell_type":"markdown","source":["Simple example that demonstrates how to use the torch.nn.Embedding layer as part of a larger model, such as a basic neural network for processing sequences. This example will cover creating an embedding layer, using it within a model, and a simple training loop. The context here is a toy example for educational purposes."],"metadata":{"id":"iMIX_l3ihzpg"}},{"cell_type":"code","source":["# Import Necessary Libraries\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim"],"metadata":{"id":"95RqquTngojv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This model will be a simple feed-forward neural network for classification purposes, with one embedding layer followed by a couple of linear layers."],"metadata":{"id":"osaVzAe2iKvZ"}},{"cell_type":"code","source":["# Simple Model with an Embedding Layer\n","class SimpleNNWithEmbedding(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n","        super(SimpleNNWithEmbedding, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n","        self.relu = nn.ReLU()\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)  # Apply embedding layer\n","        x = torch.mean(x, dim=1)  # Example of pooling/aggregating embeddings\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        return x"],"metadata":{"id":"nEHqj90kiAy-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize the Model, Loss Function, and Optimizer\n","\n","# Parameters for our model\n","vocab_size = 100  # Example vocabulary size\n","embedding_dim = 10  # Size of each embedding vector\n","hidden_dim = 16  # Hidden dimension size\n","output_dim = 2  # Output dimension size (e.g., for binary classification)\n","\n","model = SimpleNNWithEmbedding(vocab_size, embedding_dim, hidden_dim, output_dim)\n","\n","loss_function = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"],"metadata":{"id":"nfENbruOiOTY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training Loop\n","for epoch in range(1):  # Just for demo, run for more epochs\n","    for inputs, labels in data_loader:\n","        optimizer.zero_grad()  # Zero the gradients\n","        outputs = model(inputs)  # Forward pass\n","        loss = loss_function(outputs, labels)  # Compute loss\n","        loss.backward()  # Backward pass\n","        optimizer.step()  # Update weights\n","    print(f\"Epoch {epoch}, Loss: {loss.item()}\")"],"metadata":{"id":"nnYD1wS0iTjS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*    This example simplifies many aspects of training a neural network, such as data preparation and evaluation.\n","\n","*    The forward method of our model applies the embedding layer to the input indices, aggregates the embeddings (in this case, by averaging), and then passes the result through additional layers.\n","\n","*    The training loop consists of the typical steps: forward pass, loss computation, backward pass, and parameter update.\n","\n","This example should give you a starting point for understanding how to incorporate embedding layers into PyTorch models and how they can be trained. Remember to adapt and expand upon this example based on the specifics of your task and data."],"metadata":{"id":"5QtKFdLViY7Q"}},{"cell_type":"markdown","source":["# Parameters"],"metadata":{"id":"xJPDXHTwivno"}},{"cell_type":"markdown","source":["In PyTorch, `torch.nn.parameter.Parameter` is a subclass of `torch.Tensor` that is used to represent parameters of a model. Parameters are tensors that get special treatment during the training process, primarily because they are automatically added to the list of parameters (`model.parameters()`) to be optimized when training a model. This means that the optimizer will update these tensors during the backward pass.\n","\n","### Key Characteristics\n","\n","- **Trainable**: By default, parameters are considered trainable, meaning their values are adjusted through backpropagation during the training process.\n","- **Automatically Registered**: When used within a `torch.nn.Module`, any `Parameter` is automatically registered as a parameter of the module. This is done by assigning `Parameter` objects as attributes of the module.\n","- **Included in Model's Parameters**: When you call `model.parameters()`, it returns an iterator of all parameters in the model, which includes any instance of `Parameter`.\n","\n","### Usage\n","\n","The primary use of `Parameter` is to define variables that should be considered as parameters of a model—weights, biases, etc.—that will be learned during the training process.\n","\n","### Example\n","\n","When you define a custom layer or module, you might use `Parameter` to explicitly define tensors that should be treated as parameters:\n","\n","```python\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CustomLinearLayer(nn.Module):\n","    def __init__(self, input_features, output_features):\n","        super(CustomLinearLayer, self).__init__()\n","        # Define weight and bias as Parameters, making them part of the model's parameters\n","        self.weight = nn.Parameter(torch.Tensor(input_features, output_features))\n","        self.bias = nn.Parameter(torch.Tensor(output_features))\n","        \n","        # Initialize parameters (weights and biases)\n","        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))  # Example initialization\n","        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n","        bound = 1 / math.sqrt(fan_in)\n","        nn.init.uniform_(self.bias, -bound, bound)\n","\n","    def forward(self, x):\n","        # Use the parameters in a forward pass\n","        return F.linear(x, self.weight, self.bias)\n","```\n","\n","In this example, `self.weight` and `self.bias` are instances of `Parameter`, which makes them automatically trainable parameters of the `CustomLinearLayer` module. This means they will be updated by the optimizer during training.\n","\n","### Conclusion\n","\n","`torch.nn.parameter.Parameter` is a powerful tool for defining trainable parameters within PyTorch models. It ensures that parameters are automatically considered for updates during the training process, making it easier to build and train complex neural networks."],"metadata":{"id":"9uXxZudki4t_"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CustomLinearLayer(nn.Module):\n","    def __init__(self, input_features, output_features):\n","        super(CustomLinearLayer, self).__init__()\n","        # Define weight and bias as Parameters, making them part of the model's parameters\n","        self.weight = nn.Parameter(torch.Tensor(input_features, output_features))\n","        self.bias = nn.Parameter(torch.Tensor(output_features))\n","\n","        # Initialize parameters (weights and biases)\n","        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))  # Example initialization\n","        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n","        bound = 1 / math.sqrt(fan_in)\n","        nn.init.uniform_(self.bias, -bound, bound)\n","\n","    def forward(self, x):\n","        # Use the parameters in a forward pass\n","        return F.linear(x, self.weight, self.bias)"],"metadata":{"id":"_68m00qii8RO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* What is new to us is the `nn.Parameter` class.\n","* It is just a (subclass of) tensor with requires_grad set to True by default.\n","* `torch.nn.parameter.Parameter(data=None, requires_grad=True)`\n","* A method in the **base class** looks for **Parameter** in the module.If found, then it registers it to the list (in fact, dict) of parameters."],"metadata":{"id":"6KAkTsTB8orl"}},{"cell_type":"markdown","source":["# Linear"],"metadata":{"id":"kdla0I0RjYBI"}},{"cell_type":"markdown","source":["`torch.nn.Linear` is a module provided by PyTorch that applies a linear transformation to the incoming data. It's one of the most commonly used layers in neural networks for creating fully connected layers. The linear transformation it applies can be mathematically represented as \\(y = xA^T + b\\), where:\n","- \\(x\\) is the input feature vector,\n","- \\(A\\) is the weight matrix,\n","- \\(b\\) is the bias vector, and\n","- \\(y\\) is the output of the module.\n","\n","### Parameters\n","\n","The `torch.nn.Linear` module has the following main parameters:\n","\n","- `in_features`: size of each input sample (the size of \\(x\\)),\n","- `out_features`: size of each output sample (the dimension of \\(y\\)),\n","- `bias`: a boolean value indicating whether to include a bias term \\(b\\) in the transformation. The default value is `True`.\n","\n","### Usage\n","\n","Here's a basic example of how to use the `torch.nn.Linear` module:\n","\n","```python\n","import torch\n","import torch.nn as nn\n","\n","# Create a Linear layer\n","linear_layer = nn.Linear(in_features=10, out_features=5)\n","\n","# Example input (batch_size=1, in_features=10)\n","input_tensor = torch.randn(1, 10)\n","\n","# Apply the linear layer\n","output_tensor = linear_layer(input_tensor)\n","\n","print(output_tensor)\n","```\n","\n","In this example, `linear_layer` is an instance of `torch.nn.Linear` that takes inputs with 10 features and transforms them into outputs with 5 features. This is achieved by multiplying the input by a weight matrix (of size [10, 5] in this case) and adding a bias vector (of size [5]) if `bias` is `True`.\n","\n","### Key Points\n","\n","- **Weight and Bias**: The weights and bias of the linear layer are automatically initialized but can be customized post-initialization. They are accessible through `linear_layer.weight` and `linear_layer.bias`, respectively, and are instances of `torch.nn.parameter.Parameter`, meaning they are trainable parameters.\n","- **Training**: During the training process, these parameters are automatically adjusted through backpropagation to minimize the loss function.\n","- **Applications**: `torch.nn.Linear` is a fundamental building block in neural networks and can be used in various architectures, including feedforward neural networks, convolutional neural networks (for fully connected layers), and recurrent neural networks.\n","\n","`torch.nn.Linear` is a versatile and essential module in PyTorch, making it straightforward to add fully connected layers to your models."],"metadata":{"id":"3pR_40c5jZl4"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# Create a Linear layer\n","linear_layer = nn.Linear(in_features=10, out_features=5)\n","\n","# Example input (batch_size=1, in_features=10)\n","input_tensor = torch.randn(1, 10)\n","\n","# Apply the linear layer\n","output_tensor = linear_layer(input_tensor)\n","\n","print(output_tensor)"],"metadata":{"id":"B3PncU1Cji9B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Base class**\n","\n"," * The **nn** module contains all the components to **conveniently** build any deep learning architecture.\n"," * **nn.Module** provides different type of layers (called modules in pytorch) such as `linear` layer, `convolutional` layer,..\n"," * All the `modules or models` that we create must subclass this `nn.Module` baseclass\n","\n"," * They must **implement the forward** method in the subclass. The rest will be taken care by the methods defined in the base class.\n","\n"," * I really urge you to take look at the source code of **nn.Module**\n","\n"," * Let's try to reproduce one of the existing layers (modules) called Linear."],"metadata":{"id":"ib7n8dmd8UMM"}},{"cell_type":"code","source":["class LinearLayer(nn.Module):\n","\n","  def __init__(self,in_features,out_features):\n","    super(LinearLayer,self).__init__()\n","    self.in_features = in_features\n","    self.out_features = out_features\n","    self.w = nn.Parameter(torch.randn(in_features, out_features))\n","    self.b = nn.Parameter(torch.randn(out_features))\n","\n","  def forward(self,x):\n","    out = torch.matmul(x,self.w)+self.b"],"metadata":{"id":"SRBeCZOb8Xck"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# LayerNorm"],"metadata":{"id":"08yYRuU8jkhQ"}},{"cell_type":"markdown","source":["`torch.nn.LayerNorm` is a PyTorch module that applies Layer Normalization to the input data. Layer Normalization is a technique to normalize the inputs across the features for each data sample independently, which can help stabilize the learning process and lead to faster convergence. It's particularly useful in deep learning models where the normalization of activations can significantly impact performance.\n","\n","### How LayerNorm Works\n","\n","Layer Normalization works by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training sample. Unlike Batch Normalization, which normalizes across the batch dimension, Layer Normalization performs normalization for each individual sample. This makes it particularly effective in situations where batch sizes are small or vary between iterations.\n","\n","### Parameters\n","\n","`torch.nn.LayerNorm` has the following parameters:\n","\n","- `normalized_shape`: the shape of the input tensor, or a subset of the input tensor dimensions, that should be normalized. For example, if the input tensor has the shape `(N, C, H, W)` (batch size, channels, height, width), and you wish to normalize across the `C, H, W` dimensions, you would set `normalized_shape` to `(C, H, W)`.\n","- `eps`: a value added to the denominator for numerical stability. The default value is `1e-5`.\n","- `elementwise_affine`: a boolean indicating whether to learn affine parameters (scale and shift) for each feature. The default value is `True`. If `True`, it learns a scale and shift parameter for each feature dimension.\n","\n","### Usage\n","\n","Here's an example of how to use `torch.nn.LayerNorm`:\n","\n","```python\n","import torch\n","import torch.nn as nn\n","\n","# Example input tensor of shape (batch_size, num_features)\n","input_tensor = torch.randn(2, 5)\n","\n","# Apply LayerNorm\n","layer_norm = nn.LayerNorm(normalized_shape=5, elementwise_affine=True)\n","output_tensor = layer_norm(input_tensor)\n","\n","print(output_tensor)\n","```\n","\n","In this example, `layer_norm` is an instance of `torch.nn.LayerNorm` that normalizes the input tensor across its features (`num_features` dimension). The `normalized_shape` parameter is set to the number of features in the input tensor. If `elementwise_affine` is `True`, the module also learns a scale and shift for each feature dimension, further enhancing the capability of the model to fit the training data.\n","\n","### Key Points\n","\n","- **Independence from Batch Size**: Layer Normalization's effectiveness does not depend on the batch size, making it suitable for tasks with variable batch sizes or where small batch sizes are preferred.\n","- **Use Cases**: It's widely used in models where controlling the internal distribution of activations is crucial, such as recurrent neural networks (RNNs) and Transformers.\n","- **Versatility**: Layer Normalization can be applied to various types of data and model architectures, making it a versatile choice for normalization needs in deep learning models.\n","\n","Layer Normalization is a powerful tool to improve the training dynamics and stability of deep neural networks, especially in architectures where batch normalization might not be applicable or optimal."],"metadata":{"id":"eFEKRuRrkCxh"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# Example input tensor of shape (batch_size, num_features)\n","input_tensor = torch.randn(2, 5)\n","\n","# Apply LayerNorm\n","layer_norm = nn.LayerNorm(normalized_shape=5, elementwise_affine=True)\n","output_tensor = layer_norm(input_tensor)\n","\n","print(output_tensor)"],"metadata":{"id":"vxmfm6qUkEdh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Module List"],"metadata":{"id":"mNgOtSkkkGxY"}},{"cell_type":"markdown","source":["`torch.nn.ModuleList` is a PyTorch container module designed to hold a list of `nn.Module` instances. It is similar to a Python list, but with added functionalities that are specific to PyTorch, making it suitable for use within neural network architectures. One of the key features of `ModuleList` is that it correctly registers the modules contained within it as submodules of the parent module. This registration is crucial for ensuring that all parameters of the modules within the `ModuleList` are visible to PyTorch optimizers, and properly included in the model's parameter list for training.\n","\n","### Key Characteristics\n","\n","- **Automatic Registration**: When you add instances of `nn.Module` to a `ModuleList`, they are automatically registered as submodules (or components) of the parent module. This means their parameters are included in the parent's parameters, and they are properly tracked for training and updating.\n","- **Flexibility**: `ModuleList` provides a flexible way to work with lists of modules, especially when the exact number of modules you need is dynamic or when you want to apply the same operation to a series of modules in a loop.\n","\n","### Usage\n","\n","`ModuleList` is particularly useful when you have a variable number of modules that perform similar operations, or when you want to iterate over modules. Here's an example of how to use `ModuleList`:\n","\n","```python\n","import torch.nn as nn\n","\n","class MyModel(nn.Module):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        self.layers = nn.ModuleList([\n","            nn.Linear(10, 20),\n","            nn.ReLU(),\n","            nn.Linear(20, 30)\n","        ])\n","    \n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n","```\n","\n","In this example, `MyModel` contains a `ModuleList` of layers. During the forward pass, it applies each layer in the `ModuleList` to the input sequentially. This pattern is very common in neural network architectures, especially when constructing networks with variable or dynamic structures.\n","\n","### Differences from Python Lists\n","\n","While you can use a regular Python list to store modules, doing so does not automatically register the modules with the parent module, which means their parameters won't be tracked or updated during training. `ModuleList` overcomes this limitation by ensuring proper registration and parameter tracking.\n","\n","### Conclusion\n","\n","`torch.nn.ModuleList` is a useful and necessary tool for effectively managing collections of modules in PyTorch. It ensures that modules are correctly registered as part of the overall model, making it easy to define, iterate, and train complex neural network architectures."],"metadata":{"id":"Gd7bLkvVkIDI"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class MyModel(nn.Module):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        self.layers = nn.ModuleList([\n","            nn.Linear(10, 20),\n","            nn.ReLU(),\n","            nn.Linear(20, 30)\n","        ])\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x"],"metadata":{"id":"buwWqP6FkHRY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Sequential"],"metadata":{"id":"pj54NkRflFbp"}},{"cell_type":"markdown","source":["`torch.nn.Sequential` is a container module in PyTorch that sequences together other modules and layers in a specific order. It simplifies the process of building neural networks by allowing you to stack different layers and modules in the order they should be executed during the forward pass. When you input data into a `Sequential` container, it passes through all its modules sequentially, from the first to the last, with the output of one module becoming the input to the next.\n","\n","### Key Characteristics\n","\n","- **Simplicity**: It provides a clean and straightforward way to define a model by stacking layers and modules without explicitly defining the `forward` method.\n","- **Automatic Forward Pass**: The `Sequential` container automatically handles the forward pass through all its contained modules in the order they were added.\n","- **Flexibility**: While it's most commonly used for simple linear stacks of layers, you can also include modules that contain branching, pooling, or other more complex behaviors, as long as the overall data flow remains sequential.\n","\n","### Usage\n","\n","Here's an example of how to use `torch.nn.Sequential`:\n","\n","```python\n","import torch.nn as nn\n","\n","# Define a simple sequential model\n","model = nn.Sequential(\n","    nn.Linear(10, 20),\n","    nn.ReLU(),\n","    nn.Linear(20, 30),\n","    nn.ReLU(),\n","    nn.Linear(30, 10)\n",")\n","\n","# Now you can pass input data to this model directly\n","# Example input tensor\n","input_tensor = torch.randn(5, 10)  # batch size of 5, input features of 10\n","output_tensor = model(input_tensor)\n","```\n","\n","In this example, `model` is an instance of `torch.nn.Sequential` containing a stack of linear layers interspersed with ReLU activations. This model can take an input tensor and automatically apply all its layers in sequence.\n","\n","### Advantages and Limitations\n","\n","- **Advantages**: `Sequential` is great for quickly building models when the data flow is a simple linear stack of layers. It makes the code shorter and cleaner.\n","- **Limitations**: Since `Sequential` automatically defines the forward pass, it's not suitable for models that require branching, multiple inputs/outputs at different layers, or any non-linear data flow. For such models, you would need to define a custom `nn.Module` and explicitly implement the forward pass.\n","\n","### Conclusion\n","\n","`torch.nn.Sequential` is a convenient tool for defining straightforward neural networks in PyTorch. It allows for quick and clean model definition, making it ideal for many common architectures. However, for more complex models with non-sequential data flows, a custom `nn.Module` with an explicitly defined `forward` method would be necessary."],"metadata":{"id":"mfieYq10lN2V"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","# Define a simple sequential model\n","model = nn.Sequential(\n","    nn.Linear(10, 20),\n","    nn.ReLU(),\n","    nn.Linear(20, 30),\n","    nn.ReLU(),\n","    nn.Linear(30, 10)\n",")\n","\n","# Now you can pass input data to this model directly\n","# Example input tensor\n","input_tensor = torch.randn(5, 10)  # batch size of 5, input features of 10\n","output_tensor = model(input_tensor)"],"metadata":{"id":"xRhA_Jv-lSuu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Cross Entropy Loss"],"metadata":{"id":"0t2UMnt2lUS1"}},{"cell_type":"markdown","source":["`torch.nn.CrossEntropyLoss` is a loss function provided by PyTorch that is commonly used for classification tasks. It combines `nn.LogSoftmax` and `nn.NLLLoss` (negative log likelihood loss) in a single class, which makes it very convenient and computationally efficient for training models on classification problems.\n","\n","### How it Works\n","\n","- **Softmax Application**: First, it applies the softmax function to the output logits (predictions) of the model to obtain a probability distribution over classes for each sample.\n","- **Negative Log Likelihood**: Then, it computes the negative log likelihood of the true class labels given the predicted probability distribution. This measures how well the model's predictions match the true labels.\n","\n","### Characteristics\n","\n","- It expects the model outputs to be raw, unnormalized scores (also known as logits) for each class.\n","- The target labels should be indices specifying the class label for each sample (for single-label classification) or a tensor of the same shape as the input containing probabilities (for multi-label classification with soft targets).\n","\n","### Usage\n","\n","Here's an example of how to use `torch.nn.CrossEntropyLoss`:\n","\n","```python\n","import torch\n","import torch.nn as nn\n","\n","# Define the size of each input sample and the number of classes\n","input_size = 10  # number of input features\n","num_classes = 4  # number of classes\n","\n","# Example model\n","model = nn.Linear(input_size, num_classes)\n","\n","# Example input and true labels\n","input_tensor = torch.randn(3, input_size)  # batch size of 3\n","target = torch.tensor([2, 0, 1])  # true class indices\n","\n","# CrossEntropyLoss\n","criterion = nn.CrossEntropyLoss()\n","\n","# Forward pass: compute predicted outputs by passing inputs to the model\n","output = model(input_tensor)\n","\n","# Compute the loss\n","loss = criterion(output, target)\n","\n","print(loss)\n","```\n","\n","### Key Points\n","\n","- **No need for Softmax**: You do not need to apply a softmax layer to your model's output before passing it to `CrossEntropyLoss` since it's already included within the loss computation.\n","- **Target Format**: For single-label classification, the target tensor should contain the class indices. For multi-label classification with soft and hard targets, different considerations apply, and you might need to use another loss function like `torch.nn.BCEWithLogitsLoss`.\n","- **Numerical Stability**: Combining softmax and negative log likelihood in one operation is numerically more stable than applying them separately.\n","\n","`torch.nn.CrossEntropyLoss` is widely used for training neural networks on classification tasks due to its efficiency and convenience, making it a fundamental component of many machine learning pipelines in PyTorch."],"metadata":{"id":"FbMMNY7AlY3Q"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# Define the size of each input sample and the number of classes\n","input_size = 10  # number of input features\n","num_classes = 4  # number of classes\n","\n","# Example model\n","model = nn.Linear(input_size, num_classes)\n","\n","# Example input and true labels\n","input_tensor = torch.randn(3, input_size)  # batch size of 3\n","target = torch.tensor([2, 0, 1])  # true class indices\n","\n","# CrossEntropyLoss\n","criterion = nn.CrossEntropyLoss()\n","\n","# Forward pass: compute predicted outputs by passing inputs to the model\n","output = model(input_tensor)\n","\n","# Compute the loss\n","loss = criterion(output, target)\n","\n","print(loss)"],"metadata":{"id":"rsy-08CclgRH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Simple NN for MNIST"],"metadata":{"id":"-PrAvSb-9aAp"}},{"cell_type":"markdown","source":["In PyTorch, the `forward` method defines the forward pass of a neural network. When you subclass `torch.nn.Module` to create your own model, you override the `forward` method to specify how your model processes input tensors and produces output tensors. The `forward` method is where you define the operations that your model performs on its input data, such as applying layers, activation functions, and any other computations involved in producing the model's output.\n","\n","Here's a simplified overview of how it works:\n","\n","- **Definition**: You define the `forward` method as part of your subclass of `torch.nn.Module`. This method takes the input tensor(s) as its argument(s) and returns the output tensor(s).\n","- **Execution**: When you call your model on an input tensor, PyTorch automatically calls the `forward` method with the input tensor. You don't call `forward` directly; instead, you call the model itself with the input data, and the `__call__` method of `nn.Module` ensures that your `forward` method is invoked.\n","- **Computation**: Inside the `forward` method, you specify the computations to be performed on the input tensors, using other modules (like convolutional layers, linear layers, or activation functions) that you've defined as attributes in your model's constructor (`__init__` method). The sequence of operations in the `forward` method defines the computational graph for the forward pass.\n","\n","### Example\n","\n","Here's a simple example to illustrate how the `forward` method is used:\n","\n","In this example:\n","- The `SimpleNN` class inherits from `torch.nn.Module`.\n","- Two linear layers are defined in the `__init__` method.\n","- The `forward` method specifies that the input tensor `x` should first pass through the first linear layer (`fc1`), then a ReLU activation function, and finally through the second linear layer (`fc2`).\n","- The model is called with an input tensor, which triggers the `forward` method and produces an output tensor.\n","\n","The `forward` method is essential for defining the behavior of your neural network during the forward pass, determining how input data is transformed into outputs by the model."],"metadata":{"id":"8bGO6fW3Dj_U"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SimpleNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleNN, self).__init__()\n","        self.fc1 = nn.Linear(in_features=10, out_features=20)  # First layer\n","        self.fc2 = nn.Linear(in_features=20, out_features=10)  # Second layer\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))  # Apply ReLU activation function after first layer\n","        x = self.fc2(x)          # Output layer\n","        return x\n","\n","# Create an instance of the model\n","model = SimpleNN()\n","\n","# Create a dummy input tensor\n","input_tensor = torch.randn(1, 10)  # Batch size of 1, 10 features\n","\n","# Call the model on the input tensor\n","output_tensor = model(input_tensor)\n","\n","print(output_tensor)"],"metadata":{"id":"TOVu5QAE-sxV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Brief overview of the key parts and some insights:\n","\n","1. **Loading the MNIST Dataset**: You've used `torchvision.datasets.MNIST` to load the MNIST dataset, specifying that it should be transformed into tensors using `transforms.ToTensor()`. This is a common practice to prepare data for training with PyTorch models.\n","\n","2. **Inspecting the Dataset**: You've printed the classes, shape of the data, and shape of the targets to get a better understanding of the dataset structure.\n","\n","3. **Preparing the Data**: You've selected the first 1000 samples for training and normalized them by dividing by the maximum value. This normalization step is crucial for helping the neural network learn more effectively.\n","\n","4. **Building a Simple Linear Model**: You've used `nn.Linear` to create a simple linear model (fully connected layer) and examined its parameters (weights and biases). This is a basic building block for neural networks.\n","\n","5. **Making Predictions and Calculating Loss**: You've passed an input through the model, used softmax to get class probabilities, and then calculated the loss using `nn.CrossEntropyLoss`, which is a common loss function for classification tasks.\n","\n","6. **Setting Up an Optimizer**: You've initialized an SGD optimizer with the model's parameters and a learning rate. Optimizers are used to update model parameters based on gradients computed during backpropagation.\n","\n","7. **Training the Model**: You've demonstrated how to compute gradients with `.backward()`, visualize gradients, update model parameters with `optimizer.step()`, and reset gradients with `optimizer.zero_grad()`.\n","\n","8. **Building a Two-Layer Neural Network**: You've shown two approaches for constructing a neural network model in PyTorch: using `nn.Sequential` for a straightforward stack of layers and subclassing `nn.Module` for more flexibility and control over the model architecture.\n","\n","9. **Training the Two-Layer Neural Network**: You've outlined a training loop that iterates over epochs, computes loss for each training sample, updates model parameters, and zeroes gradients. Additionally, you've visualized the initial and final weights of the first layer to observe changes after training.\n","\n","This code snippet encapsulates many fundamental aspects of working with PyTorch, from data preparation and model definition to training and evaluation. It's a solid foundation for understanding how to implement and train neural networks using PyTorch."],"metadata":{"id":"zfcrSfSv-Ptd"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","\n","# Step 1: Load MNIST Dataset\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","\n","# Step 2: Define the Model\n","class FFN(nn.Module):\n","    def __init__(self):\n","        super(FFN, self).__init__()\n","        self.fc1 = nn.Linear(28*28, 128)  # Input layer to hidden layer\n","        self.fc2 = nn.Linear(128, 10)     # Hidden layer to output layer\n","\n","    def forward(self, x):\n","        x = x.view(-1, 28*28)  # Flatten the image\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x\n","\n","model = FFN()\n","\n","# Step 3: Define Loss Function and Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","\n","# Step 4: Train the Model\n","epochs = 5\n","loss_trace = []\n","\n","for epoch in range(epochs):\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n","    loss_trace.append(loss.item())\n","\n","# Plot the training loss\n","plt.plot(loss_trace)\n","plt.title('Training Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.show()\n","\n","# Note: This is a basic example for educational purposes."],"metadata":{"id":"jejl1OAU9dFw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transformer"],"metadata":{"id":"jG3MVjzYlrtQ"}},{"cell_type":"markdown","source":["`torch.nn.Transformer` is a module provided by PyTorch that implements the Transformer model architecture as described in the paper \"Attention is All You Need\" by Vaswani et al. The Transformer model has been highly influential in the field of natural language processing (NLP) and beyond, due to its effectiveness in handling sequential data without the need for recurrent neural networks (RNNs) or convolutional neural networks (CNNs). Instead, it relies entirely on a mechanism known as self-attention to draw global dependencies between input and output.\n","\n","### Key Components\n","\n","The `torch.nn.Transformer` module encapsulates the entire Transformer model, which comprises several key components:\n","\n","- **Encoder**: The encoder maps an input sequence to a sequence of continuous representations. It consists of a stack of identical layers, each containing two main sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\n","- **Decoder**: The decoder is responsible for generating the output sequence. It also consists of a stack of identical layers, but with an additional multi-head attention mechanism that attends to the encoder's output.\n","- **Positional Encoding**: Since the model does not use recurrence or convolution, a positional encoding is added to the input embeddings at the bottom of the encoder and decoder stacks to include information about the sequence order.\n","\n","### Usage\n","\n","The Transformer model is highly versatile and can be used for a wide range of tasks, such as machine translation, text summarization, and more. Here's a simplified example of how to use `torch.nn.Transformer`:\n","\n","```python\n","import torch\n","import torch.nn as nn\n","\n","# Model parameters\n","d_model = 512  # The dimensionality of the input and output of the model\n","nhead = 8  # The number of heads in the multi-head attention models\n","num_encoder_layers = 6  # The number of sub-encoder-layers in the encoder\n","num_decoder_layers = 6  # The number of sub-decoder-layers in the decoder\n","dim_feedforward = 2048  # The dimensionality of the feed-forward network model in encoder and decoder\n","\n","# Initialize the transformer model\n","transformer_model = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward)\n","\n","# Example input (src) and target (tgt) tensors\n","src = torch.rand((10, 32, d_model))  # (sequence length, batch size, feature size)\n","tgt = torch.rand((20, 32, d_model))  # (sequence length, batch size, feature size)\n","\n","# Forward pass\n","output = transformer_model(src, tgt)\n","```\n","\n","### Key Points\n","\n","- **Flexibility**: The `torch.nn.Transformer` module is highly configurable, allowing adjustments to the number of layers, heads, and other parameters to suit different tasks and datasets.\n","- **Attention Mechanism**: The core of the Transformer is the self-attention mechanism, which enables the model to weigh the importance of different parts of the input data differently.\n","- **Positional Encoding**: It's crucial to use a positional encoding (or an alternative method to incorporate sequence order information) when working with `torch.nn.Transformer`, as the model itself does not inherently understand sequence order.\n","\n","The Transformer model represents a significant advancement in sequence modeling, offering parallelizability and efficiency improvements over traditional RNN-based approaches. Its architecture has served as the foundation for numerous state-of-the-art models in NLP, including BERT, GPT, and many others."],"metadata":{"id":"g_uQYmq7lvDJ"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# Model parameters\n","d_model = 512  # The dimensionality of the input and output of the model\n","nhead = 8  # The number of heads in the multi-head attention models\n","num_encoder_layers = 6  # The number of sub-encoder-layers in the encoder\n","num_decoder_layers = 6  # The number of sub-decoder-layers in the decoder\n","dim_feedforward = 2048  # The dimensionality of the feed-forward network model in encoder and decoder\n","\n","# Initialize the transformer model\n","transformer_model = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward)\n","\n","# Example input (src) and target (tgt) tensors\n","src = torch.rand((10, 32, d_model))  # (sequence length, batch size, feature size)\n","tgt = torch.rand((20, 32, d_model))  # (sequence length, batch size, feature size)\n","\n","# Forward pass\n","output = transformer_model(src, tgt)"],"metadata":{"id":"Wm8FUgpjl6d2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Comprehensive overview of the configurable options available for the `torch.nn.Transformer` module in PyTorch. These parameters allow for extensive customization of the Transformer model to suit various tasks and data characteristics. Let's break down these parameters and their roles in the Transformer architecture:\n","\n","### Core Parameters\n","\n","- **`d_model`**: The number of expected features in the encoder/decoder inputs. This is the size of the vectors that the Transformer processes in each position of the sequence.\n","- **`nhead`**: The number of heads in the multi-head attention mechanisms. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n","- **`num_encoder_layers`**: The number of sub-encoder-layers in the encoder. Each layer consists of a multi-head self-attention mechanism followed by a position-wise fully connected feed-forward network.\n","- **`num_decoder_layers`**: The number of sub-decoder-layers in the decoder. Similar to the encoder layers, but with an additional multi-head attention mechanism that attends to the encoder's output.\n","- **`dim_feedforward`**: The dimension of the feed-forward network model in encoder and decoder intermediate layers. This is the size of the hidden layer in the feed-forward networks.\n","- **`dropout`**: The dropout rate, a regularization technique to prevent overfitting by randomly setting a fraction of the input units to 0 during training.\n","\n","### Advanced Configuration\n","\n","- **`activation`**: The activation function of the encoder/decoder's intermediate layer. Common options are \"relu\" and \"gelu\".\n","- **`custom_encoder`/`custom_decoder`**: Custom encoder or decoder to replace the default Transformer encoder or decoder.\n","- **`layer_norm_eps`**: The epsilon value for layer normalization components, to prevent division by zero.\n","- **`batch_first`**: If `True`, the input and output tensors are expected in the format `(batch, seq, feature)`, which is more aligned with other PyTorch modules. The default is `False`, meaning the format is `(seq, batch, feature)`.\n","- **`norm_first`**: Determines the order of layer normalization and attention/feedforward operations within the encoder and decoder layers.\n","- **`bias`**: If set to `False`, linear layers and layer normalization will not learn an additive bias. The default is `True`.\n","\n","### Masking and Causality\n","\n","- **`src_mask`**, **`tgt_mask`**, **`memory_mask`**: Additive masks for the source, target, and encoder output sequences. These masks allow you to prevent the model from attending to certain positions.\n","- **`src_key_padding_mask`**, **`tgt_key_padding_mask`**, **`memory_key_padding_mask`**: Masks that indicate which elements within the batch are padding elements and should not be attended to.\n","- **`src_is_causal`**, **`tgt_is_causal`**, **`memory_is_causal`**: Hints to the model indicating whether the corresponding masks are causal. A causal mask prevents the model from attending to future positions during prediction, which is crucial for tasks like language modeling.\n","\n","These parameters give users the flexibility to tailor the Transformer model to a wide range of tasks beyond simple sequence-to-sequence modeling, including but not limited to language translation, text generation, and more complex sequential tasks that require careful control over the flow of information."],"metadata":{"id":"-Ms7HFgQmVI4"}}]}