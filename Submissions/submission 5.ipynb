{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* In this assignment you will be implementing an encoder model that uses just **Masked Language Modelling (MLM)** objective.\n",
        "* We will use a simple BERT with the following modifications\n",
        "  * We just use MLM (just masking words) and **skip** NSP (Next Sentence Prediction) objective\n",
        "  * Therefore, we won't use [CLS] token\n",
        "* Again, it is absolutely fine if your loss value does not match with the one given here.\n",
        "* Just ensure that the model overfits the training data\n",
        "* You may increase the size of the training data if you want to test your implementation. In that case, we recommend you to use the tokenizer library from Hugging face\n",
        "\n"
      ],
      "metadata": {
        "id": "1-RvnrMa1aBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ],
      "metadata": {
        "id": "MYTmQEKOdNas"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cHiReOoRT-7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf07620e-d62d-48a9-f006-e67054dbe9cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdata==0.6.0\n",
            "  Downloading torchdata-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0) (2.31.0)\n",
            "Collecting torch==2.0.0 (from torchdata==0.6.0)\n",
            "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchdata==0.6.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchdata==0.6.0) (0.43.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchdata==0.6.0) (3.27.9)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading lit-18.1.2.tar.gz (161 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.0/161.0 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.0) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->torchdata==0.6.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->torchdata==0.6.0) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-18.1.2-py3-none-any.whl size=96368 sha256=c90325c23ca36015fcdb8a2b42ee74640f2425b58a7fcde81809fcf20df2fede\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/4d/9c/3e28d23c2c6fc6a9bd89c91a7b7ff775fc71a41ac9a52563e9\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.7.1\n",
            "    Uninstalling torchdata-0.7.1:\n",
            "      Successfully uninstalled torchdata-0.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchtext 0.17.1 requires torchdata==0.7.1, but you have torchdata 0.6.0 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 torchdata-0.6.0 triton-2.0.0\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.0.0\n",
            "Collecting torchtext==0.15.1\n",
            "  Downloading torchtext-0.15.1-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (2.31.0)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (1.25.2)\n",
            "Requirement already satisfied: torchdata==0.6.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (0.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0->torchtext==0.15.1) (2.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext==0.15.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext==0.15.1) (0.43.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchtext==0.15.1) (3.27.9)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchtext==0.15.1) (18.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.1) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.1) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->torchtext==0.15.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->torchtext==0.15.1) (1.3.0)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.17.1\n",
            "    Uninstalling torchtext-0.17.1:\n",
            "      Successfully uninstalled torchtext-0.17.1\n",
            "Successfully installed torchtext-0.15.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torchdata==0.6.0 # to be compatible with torch 2.0\n",
        "!pip install portalocker==2.0.0\n",
        "!pip install -U torchtext==0.15.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common Imports"
      ],
      "metadata": {
        "id": "MxKr-cWAdQ3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import one_hot\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "#text lib\n",
        "import torchtext\n",
        "\n",
        "# tokenizer\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "#build vocabulary\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# get input_ids (numericalization)\n",
        "from torchtext.transforms import VocabTransform\n",
        "\n",
        "# get embeddings\n",
        "from torch.nn import Embedding\n",
        "\n",
        "from  pprint import pprint\n",
        "from yaml import safe_load\n",
        "import copy\n",
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "kBtpv-ildSLk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "yqPv7AWKfdUW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e5e5358-9435-4905-fd0f-08beaed38cc4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize the given text"
      ],
      "metadata": {
        "id": "Qloiz5rGfvA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10"
      ],
      "metadata": {
        "id": "-uEjm4IBfv_g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer(object):\n",
        "\n",
        "  def __init__(self,text):\n",
        "    self.text = text\n",
        "    self.word_tokenizer = get_tokenizer(tokenizer=\"basic_english\",language='en')\n",
        "    self.vocab_size = None\n",
        "\n",
        "  def get_tokens(self):\n",
        "    for sentence in self.text.strip().split('\\n'):\n",
        "      yield self.word_tokenizer(sentence)\n",
        "\n",
        "  def build_vocab(self):\n",
        "    v = build_vocab_from_iterator(self.get_tokens(),\n",
        "                                  min_freq=1,specials=['<unk>','<mask>'])\n",
        "    v.set_default_index(v['<unk>']) # index of OOV\n",
        "    self.vocab_size = len(v)\n",
        "    return v\n",
        "\n",
        "  def token_ids(self):\n",
        "    v = self.build_vocab()\n",
        "    vt = VocabTransform(v)\n",
        "    num_tokens = len(self.word_tokenizer(self.text))\n",
        "    max_seq_len = np.ceil(num_tokens/batch_size)\n",
        "    data = torch.zeros(size=(1,num_tokens))\n",
        "    data = vt(self.word_tokenizer(self.text))\n",
        "    data = torch.tensor(data,dtype=torch.int64)\n",
        "    return data.reshape(batch_size,torch.tensor(max_seq_len,dtype=torch.int64))"
      ],
      "metadata": {
        "id": "grprn0yuf5NF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Best known for the invention of Error Correcting Codes, he was a true polymath who applied his mathematical and problem-solving skills to numerous disciplines.\n",
        "Reflecting on the significant benefits I received from Hamming, I decided to develop a tribute to his legacy. There has not been a previous biography of Hamming, and the few articles about him restate known facts and assumptions and leave us with open questions.\n",
        "One thought drove me as I developed this legacy project: An individual's legacy is more than a list of their attempts and accomplishments. Their tribute should also reveal the succeeding generations they inspired and enabled and what each attempted and achieved.\n",
        "This book is a unique genre containing my version of a biography that intertwines the story \"of a life\" and a multi-player memoir with particular events and turning points recalled by those, including me, who he inspired and enabled.\n",
        "Five years of research uncovered the people, places, opportunities, events, and influences that shaped Hamming. I discovered unpublished information, stories, photographs, videos, and personal remembrances to chronicle his life, which helped me put Hamming's\n",
        "legacy in the context I wanted.The result demonstrates many exceptional qualities, including his noble pursuit of excellence and helping others. Hamming paid attention to the details, his writings continue to influence, and his guidance is a timeless gift to the world.\n",
        "This biography is part of \"\"\""
      ],
      "metadata": {
        "id": "70QkLj9ageQu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tk = Tokenizer(text)"
      ],
      "metadata": {
        "id": "GC3n-zP3t_I9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = Tk.token_ids()\n",
        "print(input_ids.shape)"
      ],
      "metadata": {
        "id": "LMculIZuu6Eo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60681e56-4594-4535-87bc-4546219f22f4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 26])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We need to mask some words randomly based on the mask probability\n",
        "* The token id for the [mask] is 1\n",
        "* The function given below takes in the input ids and replaces some of the ids by 1 (token id for the [mask])\n",
        "* Since the loss is computed only over the predictions of masked tokens, we replace all non-masked input ids by -100"
      ],
      "metadata": {
        "id": "6LLxZQox6XrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getdata(ip_ids, mask_token_id, mask_prob = 0.2):\n",
        "    masked_ids = copy.deepcopy(ip_ids)\n",
        "    mask_random_idx = torch.randn_like(ip_ids, dtype = torch.float64)>(1 - mask_prob)\n",
        "    masked_ids[mask_random_idx] = mask_token_id\n",
        "    labels = copy.deepcopy(ip_ids)\n",
        "    neg_mask = ~mask_random_idx\n",
        "    labels[neg_mask] = torch.tensor(-100)\n",
        "    return (masked_ids, labels, mask_random_idx)"
      ],
      "metadata": {
        "id": "Cpo-UM-Dvu7b"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_token_id = torch.tensor([1], dtype = torch.int64)\n",
        "\n",
        "x, y, mask_mtx = getdata(input_ids, mask_token_id)\n",
        "\n",
        "print(x[0, :], '\\n', y[0,:])"
      ],
      "metadata": {
        "id": "nc9TtOBmxAiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d8afda1-16bc-4b77-cc74-ce9e9fbb2c08"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  1,   1,  69,   5,  85,   1,  63,  53,  49,   2,  20, 148,   6, 139,\n",
            "        110,  29,   1,   9,   1,   1, 112, 129,   8,   1,  59,   1]) \n",
            " tensor([  45,   23, -100, -100, -100,    7, -100, -100, -100, -100, -100, -100,\n",
            "        -100, -100, -100, -100,   36, -100,   89,    3, -100, -100, -100,   96,\n",
            "        -100,    4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now we have our inputs and labels stored in x and y,respectively\n",
        "* It is always good to test the implementation by displaying the input sentence with masked tokens"
      ],
      "metadata": {
        "id": "1DmbfCrB78OC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v = Tk.build_vocab()\n",
        "words = []\n",
        "\n",
        "for idx in x[0, :]:\n",
        "  words.append(v.vocab.get_itos()[idx.item()])\n",
        "\n",
        "print(' '.join(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7ikvzvxG91S",
        "outputId": "e1436a36-8557-489d-cf68-0cafc99a0bfe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<mask> <mask> for the invention <mask> error correcting codes , he was a true polymath who <mask> his <mask> <mask> problem-solving skills to <mask> disciplines <mask>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Also display the words that are masked"
      ],
      "metadata": {
        "id": "dHn0Qj5a8UKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "\n",
        "for idx in y[0, :]:\n",
        "  if idx != -100:\n",
        "    words.append(v.vocab.get_itos()[idx.item()])\n",
        "\n",
        "print(' '.join(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKR0ODDwIC42",
        "outputId": "8a493528-b5c5-4a18-bf37-c717e513d161"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best known of applied mathematical and numerous .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "6xQJpejJyO3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = Tk.vocab_size\n",
        "\n",
        "seq_len = x.shape[1]\n",
        "\n",
        "embed_dim = 32\n",
        "\n",
        "dmodel = embed_dim\n",
        "\n",
        "dq = torch.tensor(4)\n",
        "\n",
        "dk = torch.tensor(4)\n",
        "\n",
        "dv = torch.tensor(4)\n",
        "\n",
        "heads = torch.tensor(8)\n",
        "\n",
        "d_ff = 4 * dmodel"
      ],
      "metadata": {
        "id": "IStSC20XyQeS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "LkR1OK06xzOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MHA(nn.Module):\n",
        "\n",
        "    def __init__(self, dmodel, dq, dk, dv, heads):\n",
        "        super(MHA, self).__init__()\n",
        "        self.heads = heads\n",
        "        self.dq = dq\n",
        "        self.dk = dk\n",
        "        self.dv = dv\n",
        "        self.dmodel = dmodel\n",
        "        self.dropout = nn.Dropout(p = 0.1)\n",
        "\n",
        "        self.WQ = nn.Parameter(self.init_param(dmodel, heads * dq, seed = 43))\n",
        "        self.WK = nn.Parameter(self.init_param(dmodel, heads * dk, seed = 44))\n",
        "        self.WV = nn.Parameter(self.init_param(dmodel, heads * dv, seed = 45))\n",
        "        self.WO = nn.Parameter(self.init_param(heads * dv, dmodel, seed = 46))\n",
        "\n",
        "        self.scaling_factor = math.sqrt(dk)\n",
        "\n",
        "    def init_param(self, *size, seed):\n",
        "        torch.manual_seed(seed)\n",
        "        return torch.randn(size)\n",
        "\n",
        "    def forward(self, H = None):\n",
        "        '''\n",
        "        Input  : Size  [BS, T, dmodel]\n",
        "        Output : Size  [BS, T, dmodel]\n",
        "        '''\n",
        "        BS, T, _ = H.shape\n",
        "\n",
        "        Q = torch.matmul(H, self.WQ.T)\n",
        "        K = torch.matmul(H, self.WK.T)\n",
        "        V = torch.matmul(H, self.WV.T)\n",
        "\n",
        "        Q = Q.view(BS, T, self.heads, self.dq).transpose(1, 2)\n",
        "        K = K.view(BS, T, self.heads, self.dk).transpose(1, 2)\n",
        "        V = V.view(BS, T, self.heads, self.dv).transpose(1, 2)\n",
        "\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scaling_factor\n",
        "        attention = F.softmax(attention_scores, dim = -1)\n",
        "        # attention = self.dropout(attention)\n",
        "\n",
        "        out = torch.matmul(attention, V)\n",
        "        out = out.transpose(1, 2).contiguous().view(BS, T, -1)\n",
        "        out = torch.matmul(out, self.WO.T)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FFN(nn.Module):\n",
        "\n",
        "    def __init__(self, dmodel, d_ff):\n",
        "        super(FFN, self).__init__()\n",
        "        torch.manual_seed(47)\n",
        "        self.linear1 = nn.Linear(dmodel, d_ff)\n",
        "        self.dropout = nn.Dropout(p = 0.1)\n",
        "        torch.manual_seed(48)\n",
        "        self.linear2 = nn.Linear(d_ff, dmodel)\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        nn.init.kaiming_normal_(self.linear1.weight, mode = 'fan_out', nonlinearity = 'relu')\n",
        "        nn.init.zeros_(self.linear1.bias)\n",
        "        nn.init.kaiming_normal_(self.linear2.weight, mode = 'fan_out', nonlinearity = 'relu')\n",
        "        nn.init.zeros_(self.linear2.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Input  : Size [BS, T, dmodel]\n",
        "        Output : Size [BS, T, dmodel]\n",
        "        '''\n",
        "        x = self.linear1(x)\n",
        "        x = F.relu(x)\n",
        "        # x = self.dropout(x)\n",
        "        out = self.linear2(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Prediction(nn.Module):\n",
        "\n",
        "    def __init__(self, dmodel, vocab_size):\n",
        "        super(Prediction, self).__init__()\n",
        "        torch.manual_seed(49)\n",
        "        self.linear = nn.Linear(dmodel, vocab_size)\n",
        "        self.dropout = nn.Dropout(p = 0.1)\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        nn.init.normal_(self.linear.weight, std = 0.02)\n",
        "        nn.init.zeros_(self.linear.bias)\n",
        "\n",
        "    def forward(self, representations):\n",
        "        out = self.linear(representations)\n",
        "        # out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len = 500):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "\n",
        "\n",
        "class Embed(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(Embed, self).__init__()\n",
        "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embed = PositionalEncoding(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        token_embedding = self.token_embed(x)\n",
        "        positional_embedding = self.pos_embed(token_embedding)\n",
        "        return positional_embedding\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, dmodel, dq, dk, dv, d_ff, heads):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha = MHA(dmodel, dq, dk, dv, heads)\n",
        "        self.ffn = FFN(dmodel, d_ff)\n",
        "        self.layer_norm_1 = nn.LayerNorm(dmodel)\n",
        "        self.layer_norm_2 = nn.LayerNorm(dmodel)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Multi-head attention\n",
        "        attn_output = self.mha(x)\n",
        "        out1 = self.layer_norm_1(x + attn_output)\n",
        "        # Feed-forward network\n",
        "        ffn_output = self.ffn(out1)\n",
        "        out2 = self.layer_norm_2(out1 + ffn_output)\n",
        "        return out2"
      ],
      "metadata": {
        "id": "lwKEEtfXxWg0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, dmodel, dq, dk, dv, d_ff, heads, num_layers = 1):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embed_lookup = Embed(vocab_size, dmodel)\n",
        "        self.enc_layers = nn.ModuleList([EncoderLayer(dmodel, dq, dk, dv, d_ff, heads) for _ in range(num_layers)])\n",
        "        self.predict = Prediction(dmodel, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embed_lookup(input_ids)\n",
        "        for enc_layer in self.enc_layers:\n",
        "            x = enc_layer(x)\n",
        "        out = self.predict(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "uSzvePtJyvWj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERT(vocab_size, dmodel, dq, dk, dv, d_ff, heads, num_layers = 1)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "gwapWxHv2Djs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "HxBH1VRCB2IZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, token_ids, labels, criterion, optimizer, epochs = 1000):\n",
        "    model.train()  # Set the model to training mode\n",
        "    loss_trace = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        outputs = model(token_ids)  # Forward pass\n",
        "\n",
        "        # Flatten the outputs and labels\n",
        "        outputs_flat = outputs.view(-1, outputs.size(-1))\n",
        "        labels_flat = labels.view(-1)\n",
        "\n",
        "        # Filter out the positions where labels are -100\n",
        "        # These positions do not contribute to the loss\n",
        "        mask = labels_flat != -100\n",
        "        outputs_flat = outputs_flat[mask]\n",
        "        labels_flat = labels_flat[mask]\n",
        "\n",
        "        # Now the outputs and labels should have the same batch size\n",
        "        loss = criterion(outputs_flat, labels_flat)\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update model parameters\n",
        "\n",
        "        loss_trace.append(loss.item())\n",
        "\n",
        "        if epoch % 100 == 0:  # Print loss every 100 epochs\n",
        "            print(f'Epoch {epoch} Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "an64oZq96wvP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERT(vocab_size, dmodel, dq, dk, dv, d_ff, heads, num_layers = 1)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = -100)  # Ignore -100 labels in loss computation"
      ],
      "metadata": {
        "id": "nIjYi4mm4Jxg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, x, y, criterion, optimizer, epochs = 20000)"
      ],
      "metadata": {
        "id": "GgFE9rBKCEn7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4187840-f9d5-4ad9-fec4-cf10a5a47b89"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 5.024461269378662\n",
            "Epoch 100 Loss: 4.272457599639893\n",
            "Epoch 200 Loss: 3.7562460899353027\n",
            "Epoch 300 Loss: 3.3201756477355957\n",
            "Epoch 400 Loss: 2.8637218475341797\n",
            "Epoch 500 Loss: 2.3890035152435303\n",
            "Epoch 600 Loss: 1.9391937255859375\n",
            "Epoch 700 Loss: 1.5376886129379272\n",
            "Epoch 800 Loss: 1.1994483470916748\n",
            "Epoch 900 Loss: 0.9303851127624512\n",
            "Epoch 1000 Loss: 0.7261582612991333\n",
            "Epoch 1100 Loss: 0.5742906928062439\n",
            "Epoch 1200 Loss: 0.4608652591705322\n",
            "Epoch 1300 Loss: 0.3758087754249573\n",
            "Epoch 1400 Loss: 0.3108769357204437\n",
            "Epoch 1500 Loss: 0.26033830642700195\n",
            "Epoch 1600 Loss: 0.22075656056404114\n",
            "Epoch 1700 Loss: 0.18954791128635406\n",
            "Epoch 1800 Loss: 0.164807990193367\n",
            "Epoch 1900 Loss: 0.14484216272830963\n",
            "Epoch 2000 Loss: 0.1272018849849701\n",
            "Epoch 2100 Loss: 0.1067371740937233\n",
            "Epoch 2200 Loss: 0.09182705730199814\n",
            "Epoch 2300 Loss: 0.08094567805528641\n",
            "Epoch 2400 Loss: 0.07237010449171066\n",
            "Epoch 2500 Loss: 0.06536819040775299\n",
            "Epoch 2600 Loss: 0.05950483679771423\n",
            "Epoch 2700 Loss: 0.05449395626783371\n",
            "Epoch 2800 Loss: 0.05020752549171448\n",
            "Epoch 2900 Loss: 0.046492915600538254\n",
            "Epoch 3000 Loss: 0.0432492159307003\n",
            "Epoch 3100 Loss: 0.04038870334625244\n",
            "Epoch 3200 Loss: 0.03785337135195732\n",
            "Epoch 3300 Loss: 0.03559190034866333\n",
            "Epoch 3400 Loss: 0.03356339782476425\n",
            "Epoch 3500 Loss: 0.03173505887389183\n",
            "Epoch 3600 Loss: 0.030080288648605347\n",
            "Epoch 3700 Loss: 0.028575817123055458\n",
            "Epoch 3800 Loss: 0.02720269188284874\n",
            "Epoch 3900 Loss: 0.02594444528222084\n",
            "Epoch 4000 Loss: 0.02478836104273796\n",
            "Epoch 4100 Loss: 0.02372332289814949\n",
            "Epoch 4200 Loss: 0.02274002879858017\n",
            "Epoch 4300 Loss: 0.02182820625603199\n",
            "Epoch 4400 Loss: 0.02098199911415577\n",
            "Epoch 4500 Loss: 0.020193465054035187\n",
            "Epoch 4600 Loss: 0.019458338618278503\n",
            "Epoch 4700 Loss: 0.01877090521156788\n",
            "Epoch 4800 Loss: 0.018127020448446274\n",
            "Epoch 4900 Loss: 0.017522413283586502\n",
            "Epoch 5000 Loss: 0.01695343106985092\n",
            "Epoch 5100 Loss: 0.016417864710092545\n",
            "Epoch 5200 Loss: 0.01591266505420208\n",
            "Epoch 5300 Loss: 0.015434475615620613\n",
            "Epoch 5400 Loss: 0.014982441440224648\n",
            "Epoch 5500 Loss: 0.014553861692547798\n",
            "Epoch 5600 Loss: 0.014146432280540466\n",
            "Epoch 5700 Loss: 0.013759982772171497\n",
            "Epoch 5800 Loss: 0.013392239809036255\n",
            "Epoch 5900 Loss: 0.013042377308011055\n",
            "Epoch 6000 Loss: 0.012709352187812328\n",
            "Epoch 6100 Loss: 0.012391487136483192\n",
            "Epoch 6200 Loss: 0.0120876869186759\n",
            "Epoch 6300 Loss: 0.011797566898167133\n",
            "Epoch 6400 Loss: 0.011519987136125565\n",
            "Epoch 6500 Loss: 0.01125436183065176\n",
            "Epoch 6600 Loss: 0.01099999900907278\n",
            "Epoch 6700 Loss: 0.010755996219813824\n",
            "Epoch 6800 Loss: 0.010522091761231422\n",
            "Epoch 6900 Loss: 0.010297402739524841\n",
            "Epoch 7000 Loss: 0.010081463493406773\n",
            "Epoch 7100 Loss: 0.009873824194073677\n",
            "Epoch 7200 Loss: 0.009673655964434147\n",
            "Epoch 7300 Loss: 0.00948092807084322\n",
            "Epoch 7400 Loss: 0.009295101277530193\n",
            "Epoch 7500 Loss: 0.009115888737142086\n",
            "Epoch 7600 Loss: 0.008943060413002968\n",
            "Epoch 7700 Loss: 0.008776185102760792\n",
            "Epoch 7800 Loss: 0.00861518457531929\n",
            "Epoch 7900 Loss: 0.008459625765681267\n",
            "Epoch 8000 Loss: 0.00830920785665512\n",
            "Epoch 8100 Loss: 0.008163539692759514\n",
            "Epoch 8200 Loss: 0.008022535592317581\n",
            "Epoch 8300 Loss: 0.007886042818427086\n",
            "Epoch 8400 Loss: 0.007753891404718161\n",
            "Epoch 8500 Loss: 0.0076259043999016285\n",
            "Epoch 8600 Loss: 0.007501741871237755\n",
            "Epoch 8700 Loss: 0.007381359580904245\n",
            "Epoch 8800 Loss: 0.007264434359967709\n",
            "Epoch 8900 Loss: 0.007150780875235796\n",
            "Epoch 9000 Loss: 0.0070404368452727795\n",
            "Epoch 9100 Loss: 0.006933265831321478\n",
            "Epoch 9200 Loss: 0.006829120218753815\n",
            "Epoch 9300 Loss: 0.006727855186909437\n",
            "Epoch 9400 Loss: 0.006629388779401779\n",
            "Epoch 9500 Loss: 0.006533578038215637\n",
            "Epoch 9600 Loss: 0.006440156139433384\n",
            "Epoch 9700 Loss: 0.006349177099764347\n",
            "Epoch 9800 Loss: 0.006260608322918415\n",
            "Epoch 9900 Loss: 0.006174227222800255\n",
            "Epoch 10000 Loss: 0.00609008502215147\n",
            "Epoch 10100 Loss: 0.00600810069590807\n",
            "Epoch 10200 Loss: 0.00592805165797472\n",
            "Epoch 10300 Loss: 0.005850037094205618\n",
            "Epoch 10400 Loss: 0.005773966666311026\n",
            "Epoch 10500 Loss: 0.005699738394469023\n",
            "Epoch 10600 Loss: 0.005627167411148548\n",
            "Epoch 10700 Loss: 0.0055564045906066895\n",
            "Epoch 10800 Loss: 0.005487376824021339\n",
            "Epoch 10900 Loss: 0.005419864784926176\n",
            "Epoch 11000 Loss: 0.0053539094515144825\n",
            "Epoch 11100 Loss: 0.005289370194077492\n",
            "Epoch 11200 Loss: 0.005226330365985632\n",
            "Epoch 11300 Loss: 0.005164553411304951\n",
            "Epoch 11400 Loss: 0.005103608127683401\n",
            "Epoch 11500 Loss: 0.005044018384069204\n",
            "Epoch 11600 Loss: 0.004985847510397434\n",
            "Epoch 11700 Loss: 0.004928970709443092\n",
            "Epoch 11800 Loss: 0.004873299039900303\n",
            "Epoch 11900 Loss: 0.004818865563720465\n",
            "Epoch 12000 Loss: 0.004765496589243412\n",
            "Epoch 12100 Loss: 0.0047132172621786594\n",
            "Epoch 12200 Loss: 0.004662098363041878\n",
            "Epoch 12300 Loss: 0.004612001124769449\n",
            "Epoch 12400 Loss: 0.004562957212328911\n",
            "Epoch 12500 Loss: 0.004514859989285469\n",
            "Epoch 12600 Loss: 0.004467735532671213\n",
            "Epoch 12700 Loss: 0.004421519115567207\n",
            "Epoch 12800 Loss: 0.004376150667667389\n",
            "Epoch 12900 Loss: 0.004331701900810003\n",
            "Epoch 13000 Loss: 0.004288064781576395\n",
            "Epoch 13100 Loss: 0.004245349671691656\n",
            "Epoch 13200 Loss: 0.004203429911285639\n",
            "Epoch 13300 Loss: 0.004162290133535862\n",
            "Epoch 13400 Loss: 0.0041219028644263744\n",
            "Epoch 13500 Loss: 0.004082319792360067\n",
            "Epoch 13600 Loss: 0.004043366760015488\n",
            "Epoch 13700 Loss: 0.004005092661827803\n",
            "Epoch 13800 Loss: 0.00396754639223218\n",
            "Epoch 13900 Loss: 0.0039305901154875755\n",
            "Epoch 14000 Loss: 0.003894323483109474\n",
            "Epoch 14100 Loss: 0.003858619835227728\n",
            "Epoch 14200 Loss: 0.0038235571701079607\n",
            "Epoch 14300 Loss: 0.003789149923250079\n",
            "Epoch 14400 Loss: 0.0037552472203969955\n",
            "Epoch 14500 Loss: 0.003721933113411069\n",
            "Epoch 14600 Loss: 0.003689207136631012\n",
            "Epoch 14700 Loss: 0.0036569603253155947\n",
            "Epoch 14800 Loss: 0.003625249955803156\n",
            "Epoch 14900 Loss: 0.0035940613597631454\n",
            "Epoch 15000 Loss: 0.0035633950028568506\n",
            "Epoch 15100 Loss: 0.0035332185216248035\n",
            "Epoch 15200 Loss: 0.0035034881439059973\n",
            "Epoch 15300 Loss: 0.003474231343716383\n",
            "Epoch 15400 Loss: 0.003445435781031847\n",
            "Epoch 15500 Loss: 0.0034170972649008036\n",
            "Epoch 15600 Loss: 0.0033891811035573483\n",
            "Epoch 15700 Loss: 0.003361658426001668\n",
            "Epoch 15800 Loss: 0.0033345993142575026\n",
            "Epoch 15900 Loss: 0.003307952545583248\n",
            "Epoch 16000 Loss: 0.003281646640971303\n",
            "Epoch 16100 Loss: 0.003255775896832347\n",
            "Epoch 16200 Loss: 0.0032302620820701122\n",
            "Epoch 16300 Loss: 0.0032051431480795145\n",
            "Epoch 16400 Loss: 0.0031803830061107874\n",
            "Epoch 16500 Loss: 0.0031559860799461603\n",
            "Epoch 16600 Loss: 0.003131963312625885\n",
            "Epoch 16700 Loss: 0.0031082259956747293\n",
            "Epoch 16800 Loss: 0.0030848621390759945\n",
            "Epoch 16900 Loss: 0.003061832394450903\n",
            "Epoch 17000 Loss: 0.003039120463654399\n",
            "Epoch 17100 Loss: 0.003016694216057658\n",
            "Epoch 17200 Loss: 0.0029946179129183292\n",
            "Epoch 17300 Loss: 0.0029728496447205544\n",
            "Epoch 17400 Loss: 0.002951381029561162\n",
            "Epoch 17500 Loss: 0.0029301659669727087\n",
            "Epoch 17600 Loss: 0.0029092386830598116\n",
            "Epoch 17700 Loss: 0.0028886294458061457\n",
            "Epoch 17800 Loss: 0.0028682241681963205\n",
            "Epoch 17900 Loss: 0.0028480684850364923\n",
            "Epoch 18000 Loss: 0.002828150987625122\n",
            "Epoch 18100 Loss: 0.0028085499070584774\n",
            "Epoch 18200 Loss: 0.0027892154175788164\n",
            "Epoch 18300 Loss: 0.0027701202780008316\n",
            "Epoch 18400 Loss: 0.0027512009255588055\n",
            "Epoch 18500 Loss: 0.002732611494138837\n",
            "Epoch 18600 Loss: 0.0027142558246850967\n",
            "Epoch 18700 Loss: 0.0026960966642946005\n",
            "Epoch 18800 Loss: 0.0026781505439430475\n",
            "Epoch 18900 Loss: 0.002660498721525073\n",
            "Epoch 19000 Loss: 0.002643025480210781\n",
            "Epoch 19100 Loss: 0.0026257410645484924\n",
            "Epoch 19200 Loss: 0.002608714159578085\n",
            "Epoch 19300 Loss: 0.002591900760307908\n",
            "Epoch 19400 Loss: 0.00257528736256063\n",
            "Epoch 19500 Loss: 0.002558857435360551\n",
            "Epoch 19600 Loss: 0.0025426114443689585\n",
            "Epoch 19700 Loss: 0.0025265561416745186\n",
            "Epoch 19800 Loss: 0.0025106542743742466\n",
            "Epoch 19900 Loss: 0.002495021792128682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The loss will come around 0.02 after 20000 epochs (again, it is absolutely fine if you get a different value)\n",
        "* Let us predict the masked tokens for all the samples in the tiny dataset"
      ],
      "metadata": {
        "id": "bm6javO6DfOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  predictions = torch.argmax(model(x), dim = -1)"
      ],
      "metadata": {
        "id": "sHyMJYvVCEu2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v = Tk.build_vocab()\n",
        "masked_words = []\n",
        "predicted_words=[]\n",
        "\n",
        "for index,idx in enumerate(y.flatten()):\n",
        "  # to display only the masked tokens\n",
        "  if idx != -100:\n",
        "    masked_words.append(v.vocab.get_itos()[idx.item()])\n",
        "    predicted_words.append(v.vocab.get_itos()[predictions.flatten()[index].item()])\n",
        "\n",
        "print('Masked Words -> ')\n",
        "print(' '.join(masked_words))\n",
        "print('\\n')\n",
        "print('Predicted Words -> ')\n",
        "print(' '.join(predicted_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90vsv_m8J57q",
        "outputId": "36227f0c-6f76-4091-9515-7711e2f73acf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masked Words -> \n",
            "best known of applied mathematical and numerous . on i i to develop tribute to . there previous of , few articles restate known and assumptions us with an than their . their reveal the succeeding generations inspired and and each and achieved . is intertwines recalled he inspired and of research the i videos , which put in the and helping a the\n",
            "\n",
            "\n",
            "Predicted Words -> \n",
            "best known of applied mathematical and numerous . on i i to develop tribute to . there previous of , few articles restate known and assumptions us with an than their . their reveal the succeeding generations inspired and and each and achieved . is intertwines recalled he inspired and of research the i videos , which put in the and helping a the\n"
          ]
        }
      ]
    }
  ]
}