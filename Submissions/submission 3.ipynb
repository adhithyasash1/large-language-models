{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RsmIIsTMZBkQ",
        "1UbaTNbHac1g",
        "LxpOb7WXJuPS",
        "dtqZMexg-Ov0",
        "tjIYp4I177LY",
        "dpx5HW_bXHWX",
        "Q5BDOOj_cJlJ",
        "DyXhnsw4SPeQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n"
      ],
      "metadata": {
        "id": "RsmIIsTMZBkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In this assignment you will be using the entire transformer architecture for a translation task.\n",
        "* we will just be using one encoder layer and one decoder layer\n",
        "* You can copy the encoder and decoder modules from the previous assignments. You are going to translate a few sentences from **English to Tamil**\n",
        "  * Source language: English\n",
        "  * Target language: Tamil\n",
        "\n",
        "* You may experiment with a target language of your choice for checking the impelementation. (You may use google translate for that)\n",
        "\n",
        "* We need to install torchdata and torchtext (which take about 3 minutes to finish installing) for tokenizing the text.\n",
        "* We already defined useful functions for the tokenization of texts\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HuEecgDwXV4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata==0.6.0 # to be compatible with torch 2.0\n",
        "!pip install portalocker==2.0.0\n",
        "!pip install -U torchtext==0.15.1"
      ],
      "metadata": {
        "id": "m1TwVG-9cQmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fd9b8d4-7a9a-4186-e91b-39e234a510d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdata==0.6.0\n",
            "  Downloading torchdata-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0) (2.31.0)\n",
            "Collecting torch==2.0.0 (from torchdata==0.6.0)\n",
            "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchdata==0.6.0) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchdata==0.6.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchdata==0.6.0) (0.43.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchdata==0.6.0) (3.27.9)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0->torchdata==0.6.0)\n",
            "  Downloading lit-18.1.1.tar.gz (161 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.1/161.1 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.0) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.6.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->torchdata==0.6.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->torchdata==0.6.0) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-18.1.1-py3-none-any.whl size=96363 sha256=d229f81cf31b8c493e466574289b1e5ef39f70b90a64724752ddf4b2285f733b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/74/6b/88e95944e9f9078f1dc1c0f634a542efb4d26ecae6000ca8cf\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.7.1\n",
            "    Uninstalling torchdata-0.7.1:\n",
            "      Successfully uninstalled torchdata-0.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.0.0 which is incompatible.\n",
            "torchtext 0.17.1 requires torchdata==0.7.1, but you have torchdata 0.6.0 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 torchdata-0.6.0 triton-2.0.0\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.0.0\n",
            "Collecting torchtext==0.15.1\n",
            "  Downloading torchtext-0.15.1-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (2.31.0)\n",
            "Requirement already satisfied: torch==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (1.25.2)\n",
            "Requirement already satisfied: torchdata==0.6.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.1) (0.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->torchtext==0.15.1) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.0->torchtext==0.15.1) (2.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext==0.15.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchtext==0.15.1) (0.43.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchtext==0.15.1) (3.27.9)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->torchtext==0.15.1) (18.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.1) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.1) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0->torchtext==0.15.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->torchtext==0.15.1) (1.3.0)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.17.1\n",
            "    Uninstalling torchtext-0.17.1:\n",
            "      Successfully uninstalled torchtext-0.17.1\n",
            "Successfully installed torchtext-0.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's import all required libraries"
      ],
      "metadata": {
        "id": "qiGXHBiwcvI1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "w4nHBd0CHVX5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import one_hot\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "#text lib\n",
        "import torchtext\n",
        "\n",
        "# tokenizer\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "#build vocabulary\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# get input_ids (numericalization)\n",
        "from torchtext.transforms import VocabTransform, LabelToIndex\n",
        "\n",
        "# get embeddings\n",
        "from torch.nn import Embedding\n",
        "\n",
        "from  pprint import pprint\n",
        "from yaml import safe_load\n",
        "import copy\n",
        "import numpy as np\n",
        "import requests\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Data"
      ],
      "metadata": {
        "id": "1UbaTNbHac1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Source and target text"
      ],
      "metadata": {
        "id": "8O1Qu_Z8pBZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_text = \"\"\"The most famous ruler of ancient India was Emperor Ashoka.\n",
        "It was during his period that Buddhism spread to different parts of Asia.\n",
        "Ashoka gave up war after seeing many people grieving death after the Kalinga war.\n",
        "He embraced Buddhism and then devoted his life to spread the message of peace and dharma.\n",
        "His service for the cause of public good was exemplary.\n",
        "He was the first ruler to give up war after victory.\n",
        "He was the first to build hospitals for animals.\n",
        "He was the first to lay roads.\"\"\""
      ],
      "metadata": {
        "id": "9rv6jV3yaf1U"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tar_text = \"\"\"பண்டைய இந்திய அரசர்களில் பேரும் புகழும் பெற்ற அரசர் அசோகர் ஆவார்.\n",
        "இவரது ஆட்சியில் தான் புத்த மதம் ஆசியாவின் பல்வேறு பகுதிகளுக்குப் பரவியது.\n",
        "கலிங்கப் போருக்குப் பின் பல உயிர்கள் மடிவதைக் கண்டு வருந்தி, போர் தொடுப்பதைக் கைவிட்டார்.\n",
        "அதற்குப் பிறகு புத்த சமயத்தைத் தழுவி, அமைதியையும் அறத்தையும் பரப்புவதற்காகத் தன் வாழ்வையே அர்ப்பணித்தார்.\n",
        "பொதுமக்களுக்கு அவர் ஆற்றிய சேவை முன் மாதிரியாக விளங்கியது.\n",
        "வெற்றிக்குப் பின் போரைத் துறந்த முதல் அரசர் அசோகர்தான்.\n",
        "உலகிலேயே முதன்முதலாக விலங்குகளுக்கும் தனியே மருத்துவமனை அமைத்துத் தந்தவரும் அசோகரே ஆவார்.\n",
        " இன்றும் அவர் உருவாக்கிய சாலைகளை நாம் பயன்படுத்திக்கொண்டு இருக்கிறோம்.\"\"\""
      ],
      "metadata": {
        "id": "fFXiagnva88E"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Tokenize and build vocabulary using a simple tokenization algorithm"
      ],
      "metadata": {
        "id": "h7vjJPzJpNrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do not edit this cell\n",
        "def seq_len(seq):\n",
        "  return len(seq.strip('').split(' '))\n",
        "\n",
        "# check the maximum length of the src and target seq to decide the context length of encdoer and decoder\n",
        "src_raw_seq = src_text.strip('').split('\\n')\n",
        "src_max_seq_len =max(list(map(seq_len,src_raw_seq)))\n",
        "print('Source max_seq_length:  ',src_max_seq_len)\n",
        "\n",
        "\n",
        "tar_raw_seq = tar_text.strip('').split('\\n')\n",
        "tar_max_seq_len =max(list(map(seq_len,tar_raw_seq)))\n",
        "print('Target max_seq_length: ',tar_max_seq_len)"
      ],
      "metadata": {
        "id": "3renrPd6fh01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc07cc1-f4fc-400f-d18c-6c0b6ef7269e"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source max_seq_length:   16\n",
            "Target max_seq_length:  11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We encourage you to go through the code given below to understand the typical functionalities of Tokenizer object (If you want, you can skip)"
      ],
      "metadata": {
        "id": "4xhQw4aM66_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do not edit this cell\n",
        "class Tokenizer(object):\n",
        "\n",
        "  def __init__(self,text):\n",
        "    self.text = text\n",
        "    self.word_tokenizer = self.word_tokenizer\n",
        "    self.vocab_size = None\n",
        "    self.vocab = None\n",
        "\n",
        "  @staticmethod\n",
        "  def word_tokenizer(seq):\n",
        "    return seq.strip('').split(' ')\n",
        "\n",
        "  def get_tokens(self):\n",
        "    for sentence in self.text.strip().split('\\n'):\n",
        "      yield self.word_tokenizer(sentence)\n",
        "\n",
        "  def build_vocab(self):\n",
        "    self.vocab = build_vocab_from_iterator(self.get_tokens(),\n",
        "                                  min_freq=1,specials=['<pad>','<start>','<end>','<unk>'])\n",
        "    self.vocab.set_default_index(self.vocab['<unk>']) # index of OOV\n",
        "    self.vocab_size = len(self.vocab)\n",
        "    return self.vocab\n",
        "\n",
        "  def encode(self,sentence):\n",
        "    v = self.build_vocab()\n",
        "    vt = VocabTransform(v)\n",
        "    token_ids = vt(self.word_tokenizer(sentence))\n",
        "    # add special tokens\n",
        "    token_ids.insert(0,v.vocab.get_stoi()['<start>'])\n",
        "    token_ids.append(v.vocab.get_stoi()['<end>']) # <end>:2\n",
        "    return torch.tensor(token_ids,dtype=torch.int64)\n",
        "\n",
        "  def decode(self,ids):\n",
        "    v = self.build_vocab()\n",
        "    list_ids = ids.tolist()\n",
        "    tokens = [v.vocab.get_itos()[id] for id in list_ids]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "  def encode_batch(self,batch_size,max_seq_len):\n",
        "    batch_data = torch.zeros(size=(batch_size,max_seq_len+2)) # +2 for special tokens\n",
        "    for i,sentence in enumerate(self.text.strip('').split('\\n')):\n",
        "      token_ids = self.encode(sentence)\n",
        "      batch_data[i,0:len(token_ids)] = token_ids\n",
        "    return batch_data.type(dtype=torch.int64)\n",
        "\n"
      ],
      "metadata": {
        "id": "b2QLspwihNSB"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* It is always go to check the implementation"
      ],
      "metadata": {
        "id": "tIiCA9uppzMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8"
      ],
      "metadata": {
        "id": "BPWNsuaolNcz"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you can play with this\n",
        "src_tokenizer = Tokenizer(src_text)\n",
        "print(src_tokenizer.encode('The most famous ruler of ancient India was Emperor Ashoka.'))\n",
        "print(src_tokenizer.encode_batch(batch_size,src_max_seq_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV0wYxrFkSzr",
        "outputId": "e8f359e8-7317-43bd-a08c-8d3d7e316a18"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1, 27, 49, 39, 15,  8, 28, 24,  5, 22, 20,  2])\n",
            "tensor([[ 1, 27, 49, 39, 15,  8, 28, 24,  5, 22, 20,  2,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1, 25,  5, 36, 14, 53, 58, 11, 16,  6, 35, 50,  8, 21,  2,  0,  0,  0],\n",
            "        [ 1, 19, 40, 17, 18,  9, 56, 47, 52, 43, 32,  9,  4, 26, 61,  2,  0,  0],\n",
            "        [ 1,  7, 37, 11, 12, 59, 33, 14, 46,  6, 16,  4, 48,  8, 51, 12, 34,  2],\n",
            "        [ 1, 23, 57, 13,  4, 31,  8, 54, 42,  5, 38,  2,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1,  7,  5,  4, 10, 15,  6, 41, 17, 18,  9, 60,  2,  0,  0,  0,  0,  0],\n",
            "        [ 1,  7,  5,  4, 10,  6, 30, 44, 13, 29,  2,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [ 1,  7,  5,  4, 10,  6, 45, 55,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you can play with this\n",
        "tar_tokenizer = Tokenizer(tar_text)\n",
        "print(tar_tokenizer.encode('பண்டைய இந்திய அரசர்களில் பேரும் புகழும் பெற்ற அரசர் அசோகர் ஆவார்.'))\n",
        "print(tar_tokenizer.encode_batch(batch_size,tar_max_seq_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZBhYD7MxzpR",
        "outputId": "fc4d83b6-cbee-45db-d253-30c4395308be"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1, 44, 22, 16, 53, 51, 52,  4, 11,  6,  2])\n",
            "tensor([[ 1, 44, 22, 16, 53, 51, 52,  4, 11,  6,  2,  0,  0],\n",
            "        [ 1, 25, 20, 39,  8, 59, 19, 49, 43, 47,  2,  0,  0],\n",
            "        [ 1, 30, 55,  7, 48, 26, 58, 29, 65, 57, 41, 31,  2],\n",
            "        [ 1, 13, 50,  8, 32, 38, 14, 18, 46, 37, 66, 17,  2],\n",
            "        [ 1, 54,  5, 21, 34, 64, 61, 68,  2,  0,  0,  0,  0],\n",
            "        [ 1, 69,  7, 56, 40, 63,  4, 12,  2,  0,  0,  0,  0],\n",
            "        [ 1, 28, 62, 67, 36, 60, 15, 35, 10,  6,  2,  0,  0],\n",
            "        [ 1,  9, 23,  5, 27, 33, 42, 45, 24,  2,  0,  0,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's load the token ids of the words in the sentences of source and target languages"
      ],
      "metadata": {
        "id": "FNVpiIqDKp0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do not edit this cell\n",
        "x = src_tokenizer.encode_batch(batch_size,src_max_seq_len)\n",
        "y = tar_tokenizer.encode_batch(batch_size,tar_max_seq_len)"
      ],
      "metadata": {
        "id": "wQKgFX1xLtC3"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* we have appended zeros to sentences that are shorter than max-seq-len\n",
        "* We have to ignore computing loss over those padded tokens\n",
        "* You have to take care of that in the cell below"
      ],
      "metadata": {
        "id": "DjE9mlNOxQlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code goes here\n",
        "label = y[:, 1:]"
      ],
      "metadata": {
        "id": "z-cIRfUeLLpN"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Define the context lengths for encoder and decoder"
      ],
      "metadata": {
        "id": "k0i4jRhh5U95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do not edit this cell\n",
        "enc_ctxt_len = src_max_seq_len+2\n",
        "dec_ctxt_len = tar_max_seq_len+2"
      ],
      "metadata": {
        "id": "Z6YT0qC-MEeG"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load configuration file"
      ],
      "metadata": {
        "id": "LxpOb7WXJuPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do not edit this cell\n",
        "config_url = \"https://raw.githubusercontent.com/Arunprakash-A/LLM-from-scratch-PyTorch/main/config_files/enc_config.yml\"\n",
        "response = requests.get(config_url)\n",
        "config = response.content.decode(\"utf-8\")\n",
        "config = safe_load(config)\n",
        "pprint(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwp2oxceUguz",
        "outputId": "68737822-27d3-419e-a3c1-209bad2ee759"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': {'batch_size': 10, 'embed_dim': 32, 'seq_len': 8, 'vocab_size': 10},\n",
            " 'model': {'d_ff': 128,\n",
            "           'd_model': 32,\n",
            "           'dk': 4,\n",
            "           'dq': 4,\n",
            "           'dv': 4,\n",
            "           'n_heads': 8,\n",
            "           'n_layers': 6}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# do not edit this cell\n",
        "src_vocab_size =src_tokenizer.vocab_size\n",
        "batch_size = x.shape[0]\n",
        "embed_dim = config['input']['embed_dim']"
      ],
      "metadata": {
        "id": "VdNVN3GUUqi4"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do not edit this cell\n",
        "dq = torch.tensor(config['model']['dq'])\n",
        "dk = torch.tensor(config['model']['dk'])\n",
        "dv = torch.tensor(config['model']['dv'])\n",
        "dmodel = embed_dim\n",
        "heads = torch.tensor(config['model']['n_heads'])\n",
        "d_ff = config['model']['d_ff']"
      ],
      "metadata": {
        "id": "UPKo8KPrXDIa"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do not edit this cell\n",
        "config_url = \"https://raw.githubusercontent.com/Arunprakash-A/LLM-from-scratch-PyTorch/main/config_files/dec_config.yml\"\n",
        "response = requests.get(config_url)\n",
        "config = response.content.decode(\"utf-8\")\n",
        "config = safe_load(config)\n",
        "pprint(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFJh4tsEJva-",
        "outputId": "8809187d-9ee7-47fa-e801-d6edb6ac90cc"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': {'batch_size': 10, 'embed_dim': 32, 'seq_len': 8, 'vocab_size': 12},\n",
            " 'model': {'d_ff': 128,\n",
            "           'd_model': 32,\n",
            "           'dk': 4,\n",
            "           'dq': 4,\n",
            "           'dv': 4,\n",
            "           'n_heads': 8,\n",
            "           'n_layers': 6}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# do not edit this cell\n",
        "tar_vocab_size = tar_tokenizer.vocab_size"
      ],
      "metadata": {
        "id": "Ijfm3gfDPWs3"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Embedding\n",
        "\n",
        " * You may take the code directly from any source."
      ],
      "metadata": {
        "id": "dtqZMexg-Ov0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len = 500):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" x: Tensor, shape [batch_size, seq_len, embedding_dim] \"\"\"\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ],
      "metadata": {
        "id": "ehcB8MeD-Rbk"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embed(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(Embed, self).__init__()\n",
        "        torch.manual_seed(70)\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.dropout = nn.Dropout(p = 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.embed(x)\n",
        "        # out = self.dropout(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1PR-2vQZZOz6"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder\n",
        "\n",
        " * You can copy paste the required code from the previous assignments"
      ],
      "metadata": {
        "id": "tjIYp4I177LY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MHA(nn.Module):\n",
        "\n",
        "    def __init__(self, dmodel, dq, dk, dv, heads):\n",
        "        super(MHA, self).__init__()\n",
        "        self.heads = heads\n",
        "        self.dq = dq\n",
        "        self.dk = dk\n",
        "        self.dv = dv\n",
        "        self.dmodel = dmodel\n",
        "        self.dropout = nn.Dropout(p = 0.1)\n",
        "\n",
        "        self.WQ = nn.Parameter(self.init_param(dmodel, heads * dq, seed = 43))\n",
        "        self.WK = nn.Parameter(self.init_param(dmodel, heads * dk, seed = 44))\n",
        "        self.WV = nn.Parameter(self.init_param(dmodel, heads * dv, seed = 45))\n",
        "        self.WO = nn.Parameter(self.init_param(heads * dv, dmodel, seed = 46))\n",
        "        self.scaling_factor = math.sqrt(dk)\n",
        "\n",
        "    def init_param(self, *size, seed):\n",
        "        torch.manual_seed(seed)\n",
        "        return torch.randn(size)\n",
        "\n",
        "    def forward(self, H = None):\n",
        "        BS, T, _ = H.shape\n",
        "\n",
        "        Q = torch.matmul(H, self.WQ.T)\n",
        "        K = torch.matmul(H, self.WK.T)\n",
        "        V = torch.matmul(H, self.WV.T)\n",
        "\n",
        "        Q = Q.view(BS, T, self.heads, self.dq).transpose(1, 2)\n",
        "        K = K.view(BS, T, self.heads, self.dk).transpose(1, 2)\n",
        "        V = V.view(BS, T, self.heads, self.dv).transpose(1, 2)\n",
        "\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scaling_factor\n",
        "        attention = F.softmax(attention_scores, dim = -1)\n",
        "        # attention = self.dropout(attention)\n",
        "\n",
        "        out = torch.matmul(attention, V)\n",
        "        out = out.transpose(1, 2).contiguous().view(BS, T, -1)\n",
        "        out = torch.matmul(out, self.WO.T)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FFN(nn.Module):\n",
        "\n",
        "    def __init__(self, dmodel, d_ff):\n",
        "        super(FFN, self).__init__()\n",
        "        torch.manual_seed(47)\n",
        "        self.linear1 = nn.Linear(dmodel, d_ff)\n",
        "        self.dropout = nn.Dropout(p = 0.1)\n",
        "        torch.manual_seed(48)\n",
        "        self.linear2 = nn.Linear(d_ff, dmodel)\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        nn.init.kaiming_normal_(self.linear1.weight, mode = 'fan_out', nonlinearity = 'relu')\n",
        "        nn.init.zeros_(self.linear1.bias)\n",
        "        nn.init.kaiming_normal_(self.linear2.weight, mode = 'fan_out', nonlinearity = 'relu')\n",
        "        nn.init.zeros_(self.linear2.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = F.relu(x)\n",
        "        # x = self.dropout(x)\n",
        "        out = self.linear2(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Prediction(nn.Module):\n",
        "\n",
        "    def __init__(self, dmodel, vocab_size):\n",
        "        super(OutputLayer, self).__init__()\n",
        "        torch.manual_seed(49)\n",
        "        self.linear = nn.Linear(dmodel, vocab_size)\n",
        "        self.dropout = nn.Dropout(p = 0.1)\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        nn.init.normal_(self.linear.weight, std = 0.02)\n",
        "        nn.init.zeros_(self.linear.bias)\n",
        "\n",
        "    def forward(self, representations):\n",
        "        out = self.linear(representations)\n",
        "        # out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, dmodel, dq, dk, dv, d_ff, heads):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha = MHA(dmodel, dq, dk, dv, heads)\n",
        "        self.layer_norm_mha = nn.LayerNorm(dmodel)\n",
        "        self.ffn = FFN(dmodel, d_ff)\n",
        "        self.layer_norm_ffn = nn.LayerNorm(dmodel)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre Layer Norm\n",
        "        x_norm = self.layer_norm_mha(x)\n",
        "        attention_output = self.mha(x_norm)\n",
        "        x = x + attention_output\n",
        "\n",
        "        x_norm = self.layer_norm_ffn(x)\n",
        "        ffn_output = self.ffn(x_norm)\n",
        "        out = x + ffn_output\n",
        "        return out\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, dq, dk, dv, d_ff, heads, num_layers = 1, max_len = 500):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
        "        self.pos_encoder = PositionalEncoding(embed_dim, max_len)\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(embed_dim, dq, dk, dv, d_ff, heads) for _ in range(num_layers)])\n",
        "        self.final_layer_norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        for layer in self.encoder_layers :\n",
        "            x = layer(x)\n",
        "        x = self.final_layer_norm(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "MVxV2ROQ8Aky"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "dpx5HW_bXHWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MHCA(nn.Module):\n",
        "\n",
        "    def __init__(self, dmodel, dq, dk, dv, heads):\n",
        "        super(MHCA, self).__init__()\n",
        "        self.heads = heads\n",
        "        self.dq = dq\n",
        "        self.dk = dk\n",
        "        self.dv = dv\n",
        "        self.dmodel = dmodel\n",
        "        self.dropout = nn.Dropout(p = 0.1)\n",
        "\n",
        "        self.WQ = nn.Parameter(self.init_param(dmodel, heads * dq, seed = 43))\n",
        "        self.WK = nn.Parameter(self.init_param(dmodel, heads * dk, seed = 44))\n",
        "        self.WV = nn.Parameter(self.init_param(dmodel, heads * dv, seed = 45))\n",
        "        self.WO = nn.Parameter(self.init_param(heads * dv, dmodel, seed = 46))\n",
        "        self.scaling_factor = math.sqrt(dk)\n",
        "\n",
        "    def init_param(self, *size, seed):\n",
        "        torch.manual_seed(seed)\n",
        "        return torch.randn(size)\n",
        "\n",
        "    def forward(self, decoder_input, encoder_output):\n",
        "        BS, T_dec, _ = decoder_input.shape\n",
        "        _, T_enc, _ = encoder_output.shape\n",
        "\n",
        "        Q = torch.matmul(decoder_input, self.WQ.T).view(BS, T_dec, self.heads, -1).transpose(1, 2)\n",
        "        K = torch.matmul(encoder_output, self.WK.T).view(BS, T_enc, self.heads, -1).transpose(1, 2)\n",
        "        V = torch.matmul(encoder_output, self.WV.T).view(BS, T_enc, self.heads, -1).transpose(1, 2)\n",
        "\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scaling_factor\n",
        "        attention = F.softmax(attention_scores, dim = -1)\n",
        "        # attention = self.dropout(attention)\n",
        "\n",
        "        out = torch.matmul(attention, V).transpose(1, 2).contiguous().view(BS, T_dec, -1)\n",
        "        out = torch.matmul(out, self.WO.T)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MHMA(nn.Module):\n",
        "\n",
        "    def __init__(self, dmodel, dq, dk, dv, heads, mask = None):\n",
        "        super(MHMA, self).__init__()\n",
        "        self.heads = heads\n",
        "        self.dq = dq\n",
        "        self.dk = dk\n",
        "        self.dv = dv\n",
        "        self.dmodel = dmodel\n",
        "        self.mask = mask\n",
        "        self.dropout = nn.Dropout(p = 0.1)\n",
        "\n",
        "        self.WQ = nn.Parameter(self.init_param(heads * dq, dmodel, seed = 43))\n",
        "        self.WK = nn.Parameter(self.init_param(heads * dk, dmodel, seed = 44))\n",
        "        self.WV = nn.Parameter(self.init_param(heads * dv, dmodel, seed = 45))\n",
        "        self.WO = nn.Parameter(self.init_param(dmodel, heads * dv, seed = 46))\n",
        "        self.scaling_factor = math.sqrt(dk)\n",
        "\n",
        "    def init_param(self, *size, seed):\n",
        "        torch.manual_seed(seed)\n",
        "        return torch.randn(size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        BS, T, _ = x.shape\n",
        "        Q = torch.matmul(x, self.WQ.T).view(BS, T, self.heads, -1).transpose(1, 2)\n",
        "        K = torch.matmul(x, self.WK.T).view(BS, T, self.heads, -1).transpose(1, 2)\n",
        "        V = torch.matmul(x, self.WV.T).view(BS, T, self.heads, -1).transpose(1, 2)\n",
        "\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scaling_factor\n",
        "\n",
        "        if mask is None:\n",
        "            mask = torch.triu(torch.ones((T, T), device = x.device, dtype = torch.bool), diagonal = 1)\n",
        "            mask = mask[None, None, :, :].expand(BS, self.heads, T, T)\n",
        "            attention_scores = attention_scores.masked_fill(mask, float('-inf'))\n",
        "        else :\n",
        "            attention_scores = attention_scores + mask\n",
        "\n",
        "        attention = F.softmax(attention_scores, dim=-1)\n",
        "        # attention = self.dropout(attention)\n",
        "\n",
        "        out = torch.matmul(attention, V).transpose(1, 2).contiguous().view(BS, T, -1)\n",
        "        out = torch.matmul(out, self.WO.T)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FFN(nn.Module):\n",
        "\n",
        "    def __init__(self, dmodel, d_ff):\n",
        "        super(FFN, self).__init__()\n",
        "        torch.manual_seed(47)\n",
        "        self.linear1 = nn.Linear(dmodel, d_ff)\n",
        "        self.dropout = nn.Dropout(p = 0.1)\n",
        "        torch.manual_seed(48)\n",
        "        self.linear2 = nn.Linear(d_ff, dmodel)\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        nn.init.kaiming_normal_(self.linear1.weight, mode='fan_out', nonlinearity='relu')\n",
        "        nn.init.zeros_(self.linear1.bias)\n",
        "        nn.init.kaiming_normal_(self.linear2.weight, mode='fan_out', nonlinearity='relu')\n",
        "        nn.init.zeros_(self.linear2.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = F.relu(x)\n",
        "        # x = self.dropout(x)\n",
        "        out = self.linear2(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class OutputLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, dmodel, vocab_size):\n",
        "        super(OutputLayer, self).__init__()\n",
        "        torch.manual_seed(49)\n",
        "        self.linear = nn.Linear(dmodel, vocab_size)\n",
        "        self.dropout = nn.Dropout(p = 0.1)\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        nn.init.normal_(self.linear.weight, std = 0.02)\n",
        "        nn.init.zeros_(self.linear.bias)\n",
        "\n",
        "    def forward(self, representations):\n",
        "        out = self.linear(representations)\n",
        "        # out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, dmodel, dq, dk, dv, d_ff, heads, mask = None):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.mhma = MHMA(dmodel, dq, dk, dv, heads, mask=mask)\n",
        "        self.mhca = MHCA(dmodel, dq, dk, dv, heads)\n",
        "        self.ffn = FFN(dmodel, d_ff)\n",
        "\n",
        "        self.pre_ln_mhma = nn.LayerNorm(dmodel)\n",
        "        self.pre_ln_mhca = nn.LayerNorm(dmodel)\n",
        "        self.pre_ln_ffn = nn.LayerNorm(dmodel)\n",
        "\n",
        "    def forward(self, dec_input, enc_output, self_attn_mask = None):\n",
        "        # Pre-Layer Normalization for all layers\n",
        "        norm_mhma_in = self.pre_ln_mhma(dec_input)\n",
        "        mhma_out = self.mhma(norm_mhma_in, self_attn_mask) + dec_input\n",
        "\n",
        "        norm_mhca_in = self.pre_ln_mhca(mhma_out)\n",
        "        mhca_out = self.mhca(norm_mhca_in, enc_output) + mhma_out\n",
        "\n",
        "        norm_ffn_in = self.pre_ln_ffn(mhca_out)\n",
        "        ffn_out = self.ffn(norm_ffn_in) + mhca_out\n",
        "        return ffn_out\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, dmodel, dq, dk, dv, d_ff, heads, mask, num_layers = 1, max_len = 500):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, dmodel)\n",
        "        self.pos_encoder = PositionalEncoding(dmodel, max_len)\n",
        "        self.dec_layers = nn.ModuleList([DecoderLayer(dmodel, dq, dk, dv, d_ff, heads, mask) for _ in range(num_layers)])\n",
        "        self.final_layer_norm = nn.LayerNorm(dmodel)\n",
        "        self.output_linear = nn.Linear(dmodel, vocab_size)  # Project back to vocab size\n",
        "\n",
        "    def forward(self, enc_output, tar_token_ids):\n",
        "        tar_embed = self.embedding(tar_token_ids)\n",
        "        tar_embed = self.pos_encoder(tar_embed)\n",
        "        dec_output = tar_embed\n",
        "        for layer in self.dec_layers:\n",
        "            dec_output = layer(dec_output, enc_output)\n",
        "        dec_output = self.final_layer_norm(dec_output)\n",
        "        logits = self.output_linear(dec_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "jDZSDFHnX5MS"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate target mask\n",
        "\n",
        "  * We will be passing the causal mask for the decoder layer as one of its arguments"
      ],
      "metadata": {
        "id": "Q5BDOOj_cJlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = (torch.triu(torch.ones(dec_ctxt_len,dec_ctxt_len)) == 1).transpose(0,1)\n",
        "mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "print(mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngAlmGaccFva",
        "outputId": "c391cab8-c3c0-4695-8866-290c28fee1f8"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "DyXhnsw4SPeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, src_vocab_size, tar_vocab_size, src_seq_len, tar_seq_len, dmodel, dq, dk, dv, d_ff, heads, target_mask, num_layers = 1):\n",
        "        super(Transformer,self).__init__()\n",
        "        self.src_embeddings = nn.Embedding(src_vocab_size, embed_dim)\n",
        "        self.tar_embeddings = nn.Embedding(tar_vocab_size, embed_dim)\n",
        "        self.pos_embeddings = PositionalEncoding(dmodel)\n",
        "        self.encoder = Encoder(src_vocab_size, dmodel, dq, dk, dv, d_ff, heads, num_layers)\n",
        "        self.decoder = Decoder(tar_vocab_size, dmodel, dq, dk, dv, d_ff, heads, target_mask, num_layers)\n",
        "\n",
        "  def forward(self, src_token_ids, tar_token_ids):\n",
        "        encoder_out = self.encoder(src_token_ids) # encoder output\n",
        "        decoder_out = self.decoder(encoder_out, tar_token_ids)  # decoder output, using the encoder output and target token IDs\n",
        "        return decoder_out"
      ],
      "metadata": {
        "id": "yCug8q1GSIjU"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(src_vocab_size, tar_vocab_size, enc_ctxt_len, dec_ctxt_len, dmodel, dq, dk, dv, d_ff, heads, mask)"
      ],
      "metadata": {
        "id": "awmAIEf7kaMA"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "#optimizer = optim.SGD(model.parameters(), lr = 0.01)"
      ],
      "metadata": {
        "id": "A18BP8POk772"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(src_token_ids, tar_token_ids, labels, epochs = 1000):\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        # Exclude the <end> token for decoder input\n",
        "        decoder_input = tar_token_ids[:, :-1]\n",
        "        targets = labels\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(src_token_ids, decoder_input)\n",
        "        # Assuming your model's output is [batch_size, seq_length, vocab_size]\n",
        "\n",
        "        # Reshape for CrossEntropyLoss, if your criterion expects [batch_size * seq_length, vocab_size]\n",
        "        output = output.reshape(-1, output.shape[-1])\n",
        "        targets = targets.reshape(-1)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, targets)\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 100 == 0 :\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "yWbFM5HnlXSC"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(x, y, label, 1000)"
      ],
      "metadata": {
        "id": "HyslfJ1KZ51L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2527651-74da-4cab-c640-edd244db1cb8"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 4.5233283042907715\n",
            "Epoch 100, Loss: 1.205599308013916\n",
            "Epoch 200, Loss: 0.17830687761306763\n",
            "Epoch 300, Loss: 0.059763211756944656\n",
            "Epoch 400, Loss: 0.030649857595562935\n",
            "Epoch 500, Loss: 0.018908249214291573\n",
            "Epoch 600, Loss: 0.0129126301035285\n",
            "Epoch 700, Loss: 0.009422008879482746\n",
            "Epoch 800, Loss: 0.007192558143287897\n",
            "Epoch 900, Loss: 0.0056755393743515015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the model AutoRegressively"
      ],
      "metadata": {
        "id": "rkRq8gYolAh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def inference(model, src_token_ids, src_vocab_size, tar_vocab_size, max_len = 50):\n",
        "\n",
        "      src_token_ids = src_token_ids.long()\n",
        "      # Initialize with the <start> token\n",
        "      tar_token_ids = torch.tensor([[src_tokenizer.vocab['<start>']]], device=src_token_ids.device)\n",
        "\n",
        "      for _ in range(max_len):\n",
        "          output = model(src_token_ids, tar_token_ids)\n",
        "          next_token_logits = output[:, -1, :]  # Last token's logits\n",
        "          next_token_id = next_token_logits.argmax(dim=-1).unsqueeze(-1)\n",
        "\n",
        "          # Append predicted token\n",
        "          tar_token_ids = torch.cat([tar_token_ids, next_token_id], dim=-1)\n",
        "\n",
        "          # Break if <end> token is generated\n",
        "          if next_token_id.item() == tar_tokenizer.vocab['<end>']:\n",
        "              break\n",
        "\n",
        "      return tar_token_ids[:, 1:]  # Remove <start> token for the output"
      ],
      "metadata": {
        "id": "psLM9R44lEMf"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Modify the code below to suit your implementation\n",
        "* Display the original and translated sentence (with all the spcial tokens)\n",
        "* Note that, the second half of the second sentence is poorly translated\n",
        "*  Same goes for 3rd and 4th sentence\n",
        "* All other sentences are properly translated"
      ],
      "metadata": {
        "id": "i3GljM0lkgzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, token_ids in enumerate(x[:]):\n",
        "\n",
        "      # Ensure token_ids is in correct format for src_tokenizer.decode\n",
        "      original_sentence = src_tokenizer.decode(token_ids if isinstance(token_ids, torch.Tensor) else token_ids)\n",
        "      print(f\"Original [{i+1}]: {original_sentence}\")\n",
        "\n",
        "      # Ensure token_ids is a tensor for model inference\n",
        "      token_ids_tensor = torch.tensor(token_ids, dtype=torch.long) if not isinstance(token_ids, torch.Tensor) else token_ids\n",
        "\n",
        "      # Run inference with the tensor\n",
        "      translated_token_ids = inference(model, token_ids_tensor.unsqueeze(0), src_vocab_size, tar_vocab_size)\n",
        "\n",
        "      # Prepare translated_token_ids for tar_tokenizer.decode\n",
        "      if isinstance(translated_token_ids, torch.Tensor):\n",
        "          translated_list_ids = translated_token_ids.squeeze()\n",
        "      else:\n",
        "          translated_list_ids = translated_token_ids\n",
        "\n",
        "      # Decode the translated sequence\n",
        "      translated_sentence = tar_tokenizer.decode(translated_list_ids)\n",
        "      print(f\"Translated [{i+1}]: {translated_sentence}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb04_GlqYaMj",
        "outputId": "9a6ec3c8-bf1a-498c-d11d-54ea96878d6f"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original [1]: <start> The most famous ruler of ancient India was Emperor Ashoka. <end> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "Translated [1]: பண்டைய இந்திய அரசர்களில் பேரும் புகழும் பெற்ற அரசர் அசோகர் ஆவார். <end>\n",
            "\n",
            "Original [2]: <start> It was during his period that Buddhism spread to different parts of Asia. <end> <pad> <pad> <pad>\n",
            "Translated [2]: இவரது ஆட்சியில் தான் புத்த மதம் ஆசியாவின் பல்வேறு பகுதிகளுக்குப் பரவியது. <end>\n",
            "\n",
            "Original [3]: <start> Ashoka gave up war after seeing many people grieving death after the Kalinga war. <end> <pad> <pad>\n",
            "Translated [3]: கலிங்கப் போருக்குப் பின் பல உயிர்கள் மடிவதைக் கண்டு வருந்தி, போர் தொடுப்பதைக் கைவிட்டார். <end>\n",
            "\n",
            "Original [4]: <start> He embraced Buddhism and then devoted his life to spread the message of peace and dharma. <end>\n",
            "Translated [4]: அதற்குப் பிறகு புத்த சமயத்தைத் தழுவி, அமைதியையும் அறத்தையும் பரப்புவதற்காகத் தன் வாழ்வையே அர்ப்பணித்தார். <end>\n",
            "\n",
            "Original [5]: <start> His service for the cause of public good was exemplary. <end> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "Translated [5]: பொதுமக்களுக்கு அவர் ஆற்றிய சேவை முன் மாதிரியாக விளங்கியது. <end>\n",
            "\n",
            "Original [6]: <start> He was the first ruler to give up war after victory. <end> <pad> <pad> <pad> <pad> <pad>\n",
            "Translated [6]: வெற்றிக்குப் பின் போரைத் துறந்த முதல் அரசர் அசோகர்தான். <end>\n",
            "\n",
            "Original [7]: <start> He was the first to build hospitals for animals. <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "Translated [7]: உலகிலேயே முதன்முதலாக விலங்குகளுக்கும் தனியே மருத்துவமனை அமைத்துத் தந்தவரும் அசோகரே ஆவார். <end>\n",
            "\n",
            "Original [8]: <start> He was the first to lay roads. <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "Translated [8]:  இன்றும் அவர் உருவாக்கிய சாலைகளை நாம் பயன்படுத்திக்கொண்டு இருக்கிறோம். <end>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<start> The most famous ruler of ancient India was Emperor Ashoka. <end> <pad> <pad> <pad> <pad> <pad> <pad>\n",
        "\n",
        "<start> பண்டைய இந்திய அரசர்களில் பேரும் புகழும் பெற்ற அரசர் அசோகர் ஆவார். <pad> <pad> <pad>\n",
        "\n",
        "<start> It was during his period that Buddhism spread to different parts of Asia. <end> <pad> <pad> <pad>\n",
        "\n",
        "<start> இவரது ஆட்சியில் தான் புத்த மதம் ஆசியாவின் புத்த ஆற்றிய அசோகரே பின் <unk> பேரும்\n",
        "\n",
        "<start> Ashoka gave up war after seeing many people grieving death after the Kalinga war. <end> <pad> <pad>\n",
        "\n",
        "<start> கலிங்கப் தனியே மடிவதைக் பின் வாழ்வையே தழுவி, பெற்ற அரசர்களில் அரசர்களில் அரசர்களில் அரசர்களில் அரசர்களில்\n",
        "\n",
        "<start> He embraced Buddhism and then devoted his life to spread the message of peace and dharma. <end>\n",
        "\n",
        "<start> அதற்குப் பிறகு புத்த சமயத்தைத் தழுவி, அமைதியையும் அறத்தையும் பரப்புவதற்காகத் தன் வாழ்வையே பெற்ற அசோகர்\n",
        "\n",
        "<start> His service for the cause of public good was exemplary. <end> <pad> <pad> <pad> <pad> <pad> <pad>\n",
        "\n",
        "<start> பொதுமக்களுக்கு அவர் ஆற்றிய சேவை முன் மாதிரியாக விளங்கியது. <pad> <pad> <pad> <pad> <pad>\n",
        "\n",
        "<start> He was the first ruler to give up war after victory. <end> <pad> <pad> <pad> <pad> <pad>\n",
        "\n",
        "<start> வெற்றிக்குப் பின் போரைத் துறந்த முதல் அரசர் அசோகர்தான். <pad> <pad> <pad> <pad> <pad>\n",
        "\n",
        "<start> He was the first to build hospitals for animals. <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
        "\n",
        "<start> உலகிலேயே முதன்முதலாக விலங்குகளுக்கும் தனியே மருத்துவமனை அமைத்துத் தந்தவரும் அசோகரே ஆவார். <pad> <pad> <pad>\n",
        "\n",
        "<start> He was the first to lay roads. <end> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
        "\n",
        "<start>  இன்றும் அவர் உருவாக்கிய சாலைகளை நாம் பயன்படுத்திக்கொண்டு இருக்கிறோம். <pad> <pad> <pad> <pad>"
      ],
      "metadata": {
        "id": "Yv9GwayDp--b"
      }
    }
  ]
}